{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "robust_mean_estimation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cNhuk90x35-3",
        "hFXzkCQw5JQi",
        "OxyHyu2ADEge",
        "aCcadsj6uTz8",
        "5iNQNcsy0hr9",
        "zN12ksGI0r94",
        "_EhC4gha2cs-",
        "g9e7P0ke2elX",
        "u6LGImtT202i",
        "q0rBcBNkF-hb",
        "k6f5UggBhci5",
        "xrBUr5ldJ8Bp",
        "_PS3EaXmhUGz",
        "oZC1yVd83wI7",
        "oC5oonVz3Z-Q",
        "459YSQi1GeDt"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUY_PnV5F67A"
      },
      "source": [
        "# Robust One-Dimensional Mean Estimation\n",
        "\n",
        "<font color='green'>In this practical session, we take a look at the problem of estimating the mean of a real-valued random variable with bounded variance.\n",
        "Our main objectives are the following:\n",
        "</font>\n",
        "- <font color='green'>formulating the mean estimation problem within the framework introduced in today's lecture;</font>\n",
        "- <font color='green'>formulating a precise goal;</font>\n",
        "- <font color='green'>understanding in what cases the \"usual\" sample-mean estimator meets our goal;</font>\n",
        "- <font color='green'>motivating the need for non-asymptotic analysis;</font>\n",
        "- <font color='green'>gaining some working practice with simple concentration inequalities;</font>\n",
        "- <font color='green'>presenting the derivation of a nearly optimal mean estimator due to Olivier Catoni.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK_pwnPrh4UE"
      },
      "source": [
        "Let us now formulate our problem more precisely. We want to estimate the mean of a real-valued random variable $Z \\sim P$, where $P$ is an unknown distribution with finite variance $\\mathbf{E}_{Z \\sim P}[(Z - \\mathbf{E}Z)^{2}] \\leq \\sigma^{2} < \\infty$.\n",
        "\n",
        "To put this problem in the framework introduced in today's lectures, let $\\mathcal{A} = \\mathbb{R}$ and let $Z_{1}^{n} = (Z_{i})_{i=1}^{n}$ denote a sample on $n$ i.i.d. draws from the unkown distribution $P$. We assume that the distribution $P$ is itself a member of a restricted family of distributions with variance at most $\\sigma^{2}$, that is, $P \\in \\mathcal{P}_{\\sigma^{2}} = \\{P : \\mathbf{E}_{Z \\sim P}[(Z - \\mathbf{E}_{Z\\sim P}[Z])^{2})]\\leq \\sigma^{2}\\}$. We aim to obtain an estimator $A = A(Z_{1}, \\dots, Z_{n}) \\in \\mathcal{A}$ with a small *estimation error*\n",
        "\\begin{equation}\n",
        "  \\mathcal{E}_{P}(A) = (A - \\mathbf{E}[Z])^{2}. \\tag{1}\n",
        "\\end{equation}\n",
        "<font color='green'>**To check our understanding, the next exercise asks to show that $\\mathcal{E}_{P}(A)$ indeed matches the definition of estimation error introduced in today's lecture.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNhuk90x35-3"
      },
      "source": [
        "### Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO5BCZPm4Ckv"
      },
      "source": [
        "Show that the estimation error $\\mathcal{E}_{P}(A)$ defined in equation $(1)$ matches the definition of estimation error introduced in today's lecture.\n",
        "\n",
        "More specifically, letting $\\mathcal{A} = \\mathbb{R}$ find a loss function $\\ell : \\mathcal{A} \\times \\mathbb{R} \\to [0, \\infty)$ such that\n",
        "$$\n",
        "  \\mathcal{E}_{P}(A) = \\mathbf{E}_{Z \\sim P}[\\ell(A, Z)] - \\inf_{a \\in \\mathcal{A}} \\mathbf{E}_{Z\\sim P}[\\ell(a, Z)].\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFXzkCQw5JQi"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKTvkYff5LGg"
      },
      "source": [
        "Let $\\ell(a, z) = (a - z)^{2}$. Then, we have\n",
        "$$\n",
        "  \\inf_{a \\in \\mathcal{A}} \\mathbf{E}_{Z \\sim P}[(a - Z)^{2}] = \\mathbf{E}_{Z \\sim P}[(\\mathbf{E}[Z] - Z)^{2}].\n",
        "$$\n",
        "Conditionally on the observed samples $Z_{1}, \\dots, Z_{n}$ treat $A = A(Z_{1}, \\dots, Z_{n})$ as a fixed value. Then, it follows that\n",
        "\\begin{align*}\n",
        "  &\\mathbf{E}_{Z \\sim P}[\\ell(A, Z)] - \\inf_{a \\in \\mathcal{A}} \\mathbf{E}_{Z\\sim P}[\\ell(a, Z)]\n",
        "  \\\\\n",
        "  &= \n",
        "  \\mathbf{E}_{Z \\sim P}[(A - Z)^2] - \\mathbf{E}_{Z \\sim P}[(\\mathbf{E}[Z] - Z)^{2}]\n",
        "  \\\\\n",
        "  &= \n",
        "  \\mathbf{E}_{Z \\sim P}[(A - \\mathbf{E}[Z] + \\mathbf{E}[Z] - Z)^2] - \\mathbf{E}_{Z \\sim P}[(\\mathbf{E}[Z] - Z)^{2}]\n",
        "  \\\\\n",
        "  &= \n",
        "  \\mathbf{E}_{Z \\sim P}[(A - \\mathbf{E}[Z])^{2}]\n",
        "  +2\\mathbf{E}_{Z\\sim P}[(A - \\mathbf{E}[Z])(\\mathbf{E}[Z] - Z)]\n",
        "  \\\\\n",
        "  &=\n",
        "  \\mathbf{E}_{Z \\sim P}[(A - \\mathbf{E}[Z])^{2}]\n",
        "  +2(A - \\mathbf{E}[Z])\n",
        "     \\mathbf{E}_{Z\\sim P}[\\mathbf{E}[Z] - Z] &\\text{since }A\\text{ is independent of }Z\n",
        "  \\\\\n",
        "  &=\n",
        "  \\mathbf{E}_{Z \\sim P}[(A - \\mathbf{E}[Z])^{2}]\n",
        "  \\\\\n",
        "  &=\n",
        "  (A - \\mathbf{E}[Z])^{2}\n",
        "  \\\\\n",
        "  &=\n",
        "  \\mathcal{E}_{P}(A).\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxyHyu2ADEge"
      },
      "source": [
        "## Formulating our Goal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIYJMYW2DiZW"
      },
      "source": [
        "In order to formulate a precise goal, we begin by taking a look at a simple case\n",
        "of estimating the mean of a Gaussian random variable. Thus, for now, we fix $P \\in \\mathcal{P}_{\\sigma^{2}}$ to be the Gaussian distribution with mean $\\mu \\in \\mathbb{R}$ and variance $\\sigma^{2}$, i.e., $P = N(\\mu, \\sigma^{2})$.\n",
        "\n",
        "Run the next code cell to import the packages to be used in our simulations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAOYBxy8F6F6"
      },
      "source": [
        "import numpy as np # For manipulating vectors.\n",
        "from matplotlib import pyplot as plt # For plotting.\n",
        "from scipy.stats import norm # For computations related to the Gaussian CDF."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eZW1cTSSVkG"
      },
      "source": [
        "Consider the estimator that minimizes the empirical risk\n",
        "$$\n",
        "A^{(ERM)} = \\mathrm{argmin}_{a \\in \\mathbb{R}} R(a),\n",
        "\\quad\\text{where}\\quad R(a) = \\frac{1}{n}\\sum_{i=1}^{n}(a - Z_{i})^{2}.\n",
        "$$\n",
        "Note that $A^{(ERM)}$ is arguably the most natural choice for estimating the mean $\\mathbf{E}[Z]$ since it is equal to the *sample mean*:\n",
        "$$\n",
        "  A^{(ERM)} = \\frac{1}{n}\\sum_{i=1}^{n}Z_{i}.\n",
        "$$\n",
        "\n",
        "When $P = N(\\mu, \\sigma^{2})$, then $A^{(ERM)} \\sim N(\\mu, \\sigma^{2}/n)$\n",
        "and in particular, for any $t \\in \\mathbb{R}$ we have\n",
        "\\begin{align*}\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left(\n",
        "    \\mathcal{E}_{P}(A^{(ERM)}) \\geq t^{2}\n",
        "  \\right)\n",
        "  &=\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left(\n",
        "    \\left|A^{(ERM)} - \\mu\\right| \\geq t\n",
        "  \\right)\n",
        "  \\\\\n",
        "  &=\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left(\n",
        "    \\left|N(0, \\sigma^{2}/n)\\right| \\geq t\n",
        "  \\right)\n",
        "  \\\\\n",
        "  &=\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left(\n",
        "    \\left|N(0, 1)\\right| \\geq \\frac{\\sqrt{n}t}{\\sigma}\n",
        "  \\right)\n",
        "  \\\\\n",
        "  &= 2\\left(1 - \\Phi(\\sqrt{n}t/\\sigma)\\right), \\tag{2}\n",
        "\\end{align*}\n",
        "where $\\Phi(\\cdot)$ denotes the cumulative density function of the standard normal random variable $N(0,1)$.\n",
        "\n",
        "Notice that by the above calculations, providing upper bounds on the estimation error $\\mathcal{E}_{P}(A)$ is equivalent to providing *confidence intervals* for our estimates, i.e., upper bounds on $|A^{(ERM)} - \\mu|$ that hold with probability at least $1-\\delta$ for the chosen confidence level $\\delta \\in (0,1)$.\n",
        "We thus want to  compute the smallest value $t_{\\delta}$ such that\n",
        "$$\n",
        " \\mathbb{P}_{Z_{1}^{n}}\\left(\n",
        "    \\left|A^{(ERM)} - \\mu\\right| \\geq t_{\\delta}\n",
        "  \\right)\n",
        "  \\leq \\delta.\n",
        "$$\n",
        "For the sample mean estimator $A^{(ERM)}$ and the special case when $P$ is Gaussian, the value $t_\\delta$ can be computed via equation $(2)$ by letting\n",
        "\\begin{align*}\n",
        "  &\\delta = 2\\left(1 - \\Phi(\\sqrt{n}t_{\\delta}/\\sigma)\\right)\n",
        "  \\\\\n",
        "  \\implies&\n",
        "  t_{\\delta} = \\frac{\\Phi^{-1}(1 - \\delta/2)\\sigma}{\\sqrt{n}}.\n",
        "\\end{align*}\n",
        "In particular, we have\n",
        "$$\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left(\n",
        "    \\left|A^{(ERM)} - \\mu\\right| \\geq \\frac{\\Phi^{-1}(1 - \\delta/2)\\sigma}{\\sqrt{n}}\n",
        "  \\right)\n",
        "  \\leq \\delta.\n",
        "$$\n",
        "\n",
        "The inverse cummulative density function $\\Phi^{-1}$ of the standard normal distribution cannot be expressed in terms of elementary functions. However, the following upper bound yields an analytically convenient approximation: $\\Phi^{-1}(1 - \\delta/2) \\leq \\sqrt{2\\log(2/\\delta)}$.\n",
        "When the distribution of a random variable $Z$ is Gaussian, we can rewrite the above confidence bound into its slightly looser counterpart\n",
        "$$\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left(\n",
        "    \\left|A^{(ERM)} - \\mu\\right| \\geq \\frac{\\sigma\\sqrt{2 \\log(2/\\delta)}}{\\sqrt{n}}\n",
        "  \\right)\n",
        "  \\leq \\delta.\n",
        "$$\n",
        "The above inequality motivates the definition of $L$-sub-Gaussian mean estimators.\n",
        "\n",
        "---\n",
        "\n",
        "> **Definition**\n",
        ">\n",
        "> An estimator $A$ is called $L$-sub-Gaussian for a family of distributions $\\mathcal{P} \\subseteq \\mathcal{P}_{\\sigma^{2}}$ at confidence level $\\delta \\in (0,1)$ if\n",
        "$$\n",
        "  \\sup_{P \\in \\mathcal{P}} \\left\\{\n",
        "    \\mathbb{P}_{Z_{1}^{n} \\sim P^{n}}\\left(\n",
        "      \\left|A - \\mathbf{E}_{Z\\sim P}[Z]\\right|\n",
        "      \\geq L\\sigma\\sqrt{\\frac{\\log(2/\\delta)}{n}}\n",
        "    \\right)\n",
        "  \\right\\}\n",
        "  \\leq \\delta\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "Intuitively, an estimator is $L$-sub-Gaussian if for any distribution in some set of interest it performs as well as the sample mean estimator when the underlying distribution is Gaussian. The constant $L = \\sqrt{2}$ is the best one can hope for (see the bibliographic remarks section for references).\n",
        "\n",
        "<font color='green'>**We can thus formulate a precise goal: we want to design an estimator that is $L$-sub-Gaussian simultaneously for all distributions in $\\mathcal{P}_{\\sigma^{2}}$, for some absolute constant $L$ as close to $\\sqrt{2}$ as possible.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCcadsj6uTz8"
      },
      "source": [
        "## Code Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIsD4eXlubQk"
      },
      "source": [
        "This section sets up some code that will help us visualise the experiments to follow. \n",
        "First, we implement a class `ConfidenceInterval` that allows us to plot $L$-sub-Gaussian confidence bounds and mark estimates returned by an arbitrary estimator. If enough estimates lie outside of the desired $L$-sub-Gaussian interval, the generated plots will serve as a (high-probability) visual evidence that an estimator of interest is not $L$-sub-Gaussian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agEOhe0EQa4B"
      },
      "source": [
        "class ConfidenceInterval(object):\n",
        "  \"\"\" A class for visualizing confidence intervals. \"\"\"\n",
        "\n",
        "  def __init__(self, mu, sigma, n):\n",
        "    self.mu = mu\n",
        "    self.sigma = sigma\n",
        "    self.n = n\n",
        "\n",
        "    self.fig, self.ax = plt.subplots(figsize=(15,3))\n",
        "    self.ax.set_yticks([])\n",
        "    xmin = mu - 2.5 * self.get_subgaussian_deviations(delta=1e-5)\n",
        "    xmax = mu + 2.5 * self.get_subgaussian_deviations(delta=1e-5) \n",
        "    self.set_xaxis_limits(xmin, xmax)\n",
        "    self.ax.set_ylim(-0.02, 0.05)\n",
        "\n",
        "  def get_gaussian_deviations(self, delta):\n",
        "    \"\"\" Returns the exact deviations of the sample mean estimator when the\n",
        "    underlying distribution is Gaussian. \"\"\"\n",
        "    return self.sigma * np.sqrt(1.0/self.n) * norm.ppf(1.0 - delta/2)\n",
        "\n",
        "  def get_subgaussian_deviations(self, delta, L=np.sqrt(2)):\n",
        "    \"\"\" Returns L-sub-Gaussian confidence intervals. \"\"\"\n",
        "    return self.sigma * L * np.sqrt(np.log(2.0/delta)/self.n)\n",
        "\n",
        "  def mark_deviations(self, deviation, **kwargs):\n",
        "    \"\"\" Marks deviations of the desired size on the axis object self.ax. \"\"\"\n",
        "    self.ax.vlines(x=self.mu-deviation, ymin=-0.01, ymax=0.01, **kwargs)\n",
        "    self.ax.vlines(x=self.mu+deviation, ymin=-0.01, ymax=0.01, **kwargs)\n",
        "\n",
        "  def mark_gaussian_deviations(self, delta):\n",
        "    \"\"\" Marks the exact Gaussian deviations on the axis object self.ax. \"\"\"\n",
        "    deviations = self.get_gaussian_deviations(delta)\n",
        "    self.mark_deviations(deviations, color='red')\n",
        "\n",
        "  def mark_subgaussian_deviations(self, delta, L=np.sqrt(2)):\n",
        "    \"\"\" Marks L-sub-Gaussian deviations on the axis object self.ax. \"\"\"\n",
        "    deviations = self.get_subgaussian_deviations(delta, L)\n",
        "    self.mark_deviations(deviations, color='purple')\n",
        "\n",
        "  def mark_estimates(self, estimates):\n",
        "    \"\"\" Given a numpy array of estimates returned by some estimator, marks\n",
        "    the estimates on the axis object self.ax. \"\"\"\n",
        "    self.ax.scatter(estimates, np.zeros_like(estimates), s=20.0, color='C0')\n",
        "    hist, bin_edges = np.histogram(estimates, bins=50)\n",
        "    hist = hist/np.max(hist) * 0.045 # Make hist fit to plot.\n",
        "    plt.bar(bin_edges[:-1], hist, width=np.diff(bin_edges), align='edge',\n",
        "            alpha=0.3)\n",
        "    \n",
        "    # Adjust the displayed axis limits if needed.\n",
        "    if np.max(estimates) > self.xmax:\n",
        "      self.xmax = np.max(estimates)\n",
        "    if np.min(estimates) < self.xmin:\n",
        "      self.xmin = np.min(estimates)\n",
        "    self.set_xaxis_limits(self.xmin, self.xmax)\n",
        "\n",
        "  def mark_true_mean(self):\n",
        "    \"\"\" Marks the true mean mu = E[Z] with an orange dot. \"\"\"\n",
        "    self.ax.scatter(x=self.mu, y=0.0, s=30.0, color='C1')\n",
        "\n",
        "  def set_xaxis_limits(self, xmin, xmax):\n",
        "    self.xmin = xmin\n",
        "    self.xmax = xmax\n",
        "    self.ax.set_xlim(xmin, xmax)\n",
        "    self.ax.hlines(y=0,xmin=xmin,xmax=xmax,linestyle='solid',linewidth=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y64w2kHptcdy"
      },
      "source": [
        "In the following code cell, we plot exact Gaussian confidence intervals marked by red vertical lines, as well as $\\sqrt{2}$-sub-Gaussian confidence intervals marked by purple vertical lines. We fix $\\delta = 10^{-3}$ and draw $\\delta^{-1} = 10^3$ independent samples of the random variable $A^{(ERM)}$, each computed on $n=100$ independent standard normal random variables. We then plot the histogram resulting from the $10^3$ draws of $A^{(ERM)}$.\n",
        "\n",
        "Notice that since $A^{(ERM)}$ is indeed $\\sqrt{2}$-sub-Gaussian with respect to the family of Gaussian distributions with variance at most $\\sigma^{2}$, in the limit $\\delta \\to 0$ the probability of observing $k$ or more blue points outside of the purple-marked confidence interval is upper bounded by the probability that a $\\mathrm{Poisson(1)}$ distributed random variable is greater or equal than $k$. Hence, observing more than just a few points outside of the plotted $L$-sub-Gaussian confidence interval would serve as a strong indication that the estimator used to estimate the mean is not $L$-sub-Gaussian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Raz4_-5VgIu"
      },
      "source": [
        "delta = 1e-3\n",
        "estimates = []\n",
        "\n",
        "mu=0.0\n",
        "sigma=1.0\n",
        "n=100\n",
        "\n",
        "for i in range(int(1.0/delta)):\n",
        "  Z = np.random.normal(loc=mu, scale=sigma, size=(n,))\n",
        "  estimates.append(np.mean(Z))\n",
        "estimates = np.array(estimates)\n",
        "\n",
        "confidence_interval = ConfidenceInterval(mu, sigma, n)\n",
        "confidence_interval.mark_gaussian_deviations(delta)\n",
        "confidence_interval.mark_subgaussian_deviations(delta)\n",
        "confidence_interval.mark_estimates(estimates)\n",
        "confidence_interval.mark_true_mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuFk-G0JdVcm"
      },
      "source": [
        "We will want to repeat simulations such as the one performed above for different estimators and for different distributions. For this reason, we introduce abstract classes `Estimator` and `Distribution` and provide implementations for the `SampleMean` estimator as well as `GaussianDistribution`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vojnj6mOwqPj"
      },
      "source": [
        "class Estimator(object):\n",
        "  \"\"\" A base class for mean estimators. \"\"\"\n",
        "\n",
        "  def estimate_mean(self, Z):\n",
        "    \"\"\" :Z: A numpy array of i.i.d. samples drawn from some unkown distribution.\n",
        "        :returns: An estimate of the mean E_{Z \\sim P}[Z].\n",
        "    \"\"\"\n",
        "    raise NotImplementedError('The estimate_mean function should be '\n",
        "                              'implemented by a subclass.')\n",
        "    \n",
        "class SampleMean(Estimator):\n",
        "  \"\"\" The sample mean estimator. \"\"\"\n",
        "\n",
        "  def estimate_mean(self, Z):\n",
        "    return np.mean(Z)\n",
        "\n",
        "\n",
        "class Distribution(object):\n",
        "  \"\"\" A base class for distributions. \"\"\"\n",
        "\n",
        "  def __init__(self, mu, sigma):\n",
        "    \"\"\" :mu: The mean of this distribution.\n",
        "        :sigma: The standard deviation of this distribution.\n",
        "    \"\"\"\n",
        "    self.mu = mu\n",
        "    self.sigma = sigma\n",
        "\n",
        "  def generate_samples(self, n):\n",
        "    \"\"\" Returns n independent samples drawn from this distribution. \"\"\"\n",
        "    raise NotImplementedError('The generate_samples function should be '\n",
        "                              'implemented by a subclass.')    \n",
        "    \n",
        "    \n",
        "class GaussianDistribution(Distribution):\n",
        "  \"\"\" Gaussian distribution wrapped in the class Distribution. \"\"\"\n",
        "\n",
        "  def generate_samples(self, n):\n",
        "    return np.random.normal(loc=self.mu, scale=self.sigma, size=(n,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H92FvBn4OQw"
      },
      "source": [
        "def verify_subgaussianity(estimator, distribution, delta, L, n=100, seed=None):\n",
        "  \"\"\" This function marks an L-sub-Gaussian interval at confidence level delta\n",
        "  and samples 1/delta independent outputs obtained via the given estimator on\n",
        "  the given distribution with sample size n. \"\"\"\n",
        "\n",
        "  if seed is not None:\n",
        "    np.random.seed(seed)\n",
        "\n",
        "  n_runs = int(1.0/delta)\n",
        "  estimates = np.zeros(n_runs)\n",
        "  for idx in range(n_runs):\n",
        "    Z = distribution.generate_samples(n)\n",
        "    estimates[idx] = estimator.estimate_mean(Z)\n",
        "\n",
        "  confidence_interval = ConfidenceInterval(\n",
        "      distribution.mu, distribution.sigma, n)\n",
        "  confidence_interval.mark_subgaussian_deviations(delta, L)\n",
        "  confidence_interval.mark_estimates(estimates)\n",
        "  confidence_interval.mark_true_mean()\n",
        "  confidence_interval.ax.set_title(r'$\\delta = $' + str(delta), size=20)\n",
        "\n",
        "  return confidence_interval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLQcwQrW6uc5"
      },
      "source": [
        "In the below code cell we show an example simulation using the above function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvdCT8qbY7f0"
      },
      "source": [
        "for delta in [1e-2, 1e-3, 1e-4]:\n",
        "  verify_subgaussianity(\n",
        "      estimator=SampleMean(),\n",
        "      distribution=GaussianDistribution(mu=0.0, sigma=1.0),\n",
        "      delta=delta,\n",
        "      L = np.sqrt(2),\n",
        "      n = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iNQNcsy0hr9"
      },
      "source": [
        "## Is the Sample Mean Estimator L-Sub-Gaussian?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KweSC60b0onX"
      },
      "source": [
        "We have now formulated our goal: we aim to obtain an estimator $A$ that is $L$-sub-Gaussian for the class of distributions $\\mathcal{P}_{\\sigma^{2}}$ with\n",
        "variance bounded by $\\sigma^{2}$. We have also seen that the sample mean estimator $A^{(ERM)}$ is by definition $\\sqrt{2}$-sub-Gaussian for the normal distribution $N(\\mu, \\sigma^{2})$. \n",
        "<font color='green'>**In the next exercise, we show that the sample mean estimator $A^{(ERM)}$ is $\\sqrt{2}$-sub-Gaussian for the family of $\\sigma^{2}$-sub-Gaussian distributions. Intuitively, this is a family of distributions whose tails decay as fast as those of the normally distributed random variable with variance $\\sigma^{2}$.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN12ksGI0r94"
      },
      "source": [
        "### Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erSm55DN0uaO"
      },
      "source": [
        "Define a family of $\\sigma^{2}$-sub-Gaussian distributions by\n",
        "$$\n",
        "  \\mathcal{P}_{\\sigma^{2}}^{\\text{sub-Gaussian}}\n",
        "  =\n",
        "  \\{\n",
        "    P \\in \\mathcal{P}_{\\sigma^{2}} :\n",
        "    \\forall \\lambda \\in \\mathbb{R} \\text{ we have }\n",
        "    \\mathbf{E}_{Z \\sim P}[e^{\\lambda(Z - \\mathbf{E}[Z])}] \\leq e^{\\lambda^{2}\\sigma^{2}/2}\n",
        "  \\}.\n",
        "$$\n",
        "\n",
        "Prove that the sample mean estimator $A^{(ERM)}$ is $\\sqrt{2}$-sub-Gaussian for the famility of distributions $\\mathcal{P}_{\\sigma^{2}}^{\\text{sub-Gaussian}}$ uniformly over all confidence levels $\\delta \\in (0,1)$. That is, show that for any $n \\geq 1$ and any $\\delta \\in (0,1)$ the following holds:\n",
        "$$\n",
        "  \\sup_{P \\in \\mathcal{P}_{\\sigma^{2}}^{\\text{sub-Gaussian}}}\n",
        "  \\left\\{\n",
        "      \\mathbb{P}_{Z_{1}^{n} \\sim P^{n}}\\left(\n",
        "        \\left|A^{(ERM)} - \\mathbf{E}_{Z \\sim P}[Z]\\right|\n",
        "        \\geq \\sqrt{\\frac{2 \\sigma^{2} \\log(2/\\delta)}{n}}\n",
        "      \\right)\n",
        "  \\right\\}\n",
        "  \\leq \\delta.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EhC4gha2cs-"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge9-WivSJrVM"
      },
      "source": [
        "Fix any distribution $P \\in \\mathcal{P}_{\\sigma^{2}}^{\\text{sub-Gaussian}}$. Using Chernoff's method we have for any $t > 0$:\n",
        "\\begin{align*}\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left(\n",
        "    A^{(ERM)} - \\mathbf{E}[Z] \\geq t\n",
        "  \\right)\n",
        "  &\\leq\n",
        "  \\inf_{\\lambda > 0} \\left\\{\n",
        "    \\frac{\\mathbf{E}_{Z_{1}^{n}}[e^{\\lambda(A^{(ERM)} - \\mathbf{E}[Z])}]}\n",
        "         {e^{\\lambda t}} \\right\\}\n",
        "  \\\\\n",
        "  &=\n",
        "  \\inf_{\\lambda > 0} \\left\\{\n",
        "    \\frac{\\prod_{i=1}^{n}\\mathbf{E}_{Z_{i}}[e^{\\frac{\\lambda}{n}(Z_{i} - \\mathbf{E}[Z])}]}\n",
        "         {e^{\\lambda t}} \\right\\}\n",
        "  \\\\\n",
        "  &\\leq\n",
        "  \\inf_{\\lambda > 0} \\left\\{\n",
        "    \\frac{\\prod_{i=1}^{n}\n",
        "         e^{\\left(\\frac{\\lambda}{n}\\right)^{2}\\sigma^{2}/2}}\n",
        "         {e^{\\lambda t}} \\right\\}\n",
        "  & \\text{since }Z\\text{ is }\\sigma^{2}\\ \\text{-sub-Gaussian}.\n",
        "  \\\\\n",
        "  &\\leq\n",
        "  \\inf_{\\lambda > 0} \\left\\{\n",
        "         e^{\\lambda^{2}\\sigma^{2}/(2n) - \\lambda t}\n",
        "         \\right\\}\n",
        "  \\\\\n",
        "  &= e^{-nt^{2}/(2\\sigma^{2})}.\n",
        "\\end{align*}\n",
        "Plugging in\n",
        "$t = \\sqrt{\\frac{2\\sigma^{2}\\log(2/\\delta)}{n}}$ we obtain\n",
        "$$\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\n",
        "  \\left(\n",
        "    A^{(ERM)} - \\mathbf{E}[Z]\n",
        "    \\geq\n",
        "    \\sqrt{\\frac{2\\sigma^{2}\\log(2/\\delta)}{n}}\n",
        "  \\right)\n",
        "  \\leq \\frac{\\delta}{2}.\n",
        "$$\n",
        "Via an identical argument we also have\n",
        "$$\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\n",
        "  \\left(\n",
        "    \\mathbf{E}[Z] - A^{(ERM)}\n",
        "    \\geq\n",
        "    \\sqrt{\\frac{2\\sigma^{2}\\log(2/\\delta)}{n}}\n",
        "  \\right)\n",
        "  \\leq \\frac{\\delta}{2}.\n",
        "$$\n",
        "The result follows by the union bound applied to the two inequalities above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9e7P0ke2elX"
      },
      "source": [
        "### Exercise 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jlnxw5E-xlLo"
      },
      "source": [
        "In the previous exercise we showed that the sample mean estimator\n",
        "$A^{(ERM)}$ is $\\sqrt{2}$-sub-Gaussian for the family of $\\sigma^{2}$-sub-Gaussian distributions, a subset of distributions with bounded variance whose tails decay approximately as fast as those of a Gaussian random variable with variance $\\sigma^{2}$. However, it does not mean that the sample mean estimator is uniformly sub-Gaussian for *all* distributions with variance at most $\\sigma^{2}$, i.e., distributions in the set $\\mathcal{P}_{\\sigma^{2}}$.\n",
        "\n",
        "One might argue via the Central Limit Theorem that for any $\\delta \\in (0,1)$\n",
        "we have\n",
        "$$\n",
        "  \\forall P \\in \\mathcal{P}_{\\sigma^{2}}\n",
        "  \\quad\n",
        "  \\mathbb{P}_{Z_{1}^{n} \\sim P^{n}}\\left(\n",
        "    \\left|\n",
        "      A^{(ERM)} - \\mathbf{E}_{Z \\sim P}[Z]\n",
        "    \\right|\n",
        "    \\geq \n",
        "    \\frac{\\Phi^{-1}(1 - \\delta/2)\\sigma}{\\sqrt{n}}\n",
        "  \\right)\n",
        "  \\to \\delta\n",
        "  \\text{ as }\n",
        "  n \\to \\infty,\n",
        "$$\n",
        "where recall that $\\Phi$ denotes the cumulative density function of the standard normal random variable.\n",
        "<font color='green'>\n",
        "**However, the Central Limit Theorem does not provide uniform non-asymptotic bounds (i.e., it is not clear for what values of the sample size $n$ the CLT \"kicks in\") and thus it is insufficient to establish that the sample mean estimator is sub-Gaussian for the family $\\mathcal{P}_{\\sigma^{2}}$. In the following exercice we show that the sample mean estimator is not $L$-sub-Gaussian for any constant $L > 0$.**\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnw60Kpb2gzX"
      },
      "source": [
        "- **Exercise 3.1.** Using Chebyshev's inequality find some $t_{\\delta} > 0$ such that for any $\\delta \\in (0,1)$ the sample mean estimator satisfies\n",
        "$$\n",
        "  \\sup_{P \\in \\mathcal{P}_{\\sigma^{2}}}\n",
        "  \\left\\{\n",
        "      \\mathbb{P}_{Z_{1}^{n} \\sim P^{n}}\\left(\n",
        "        \\left|\n",
        "          A^{(ERM)} - \\mathbf{E}_{Z \\sim P}[Z]\n",
        "        \\right|\n",
        "        \\geq t_{\\delta}\n",
        "      \\right)\n",
        "  \\right\\} \\leq \\delta.\n",
        "$$\n",
        "- **Exercise 3.2.** In the following code cell, implement a distribution $P \\in \\mathcal{P}_{\\sigma^{2}}$ for which the sample mean-estimator fails to be $\\sqrt{2}$-sub-Gaussian. Verify your construction visually in the generated plot by running the below code cell (see further comments therein).\n",
        "- **Exercise 3.3.** Prove that there exists some absolute constant $c > 0$  and $n_{0} \\in \\mathbb{N}$ such that the following is true for any $n \\geq n_{0}$ and any confidence level $\\delta \\in (0,1)$:\n",
        "$$\n",
        "  \\sup_{P \\in \\mathcal{P}_{\\sigma^{2}}}\n",
        "  \\left\\{\n",
        "      \\mathbb{P}_{Z_{1}^{n} \\sim P^{n}}\\left(\n",
        "        \\left|\n",
        "          A^{(ERM)} - \\mathbf{E}_{Z \\sim P}[Z]\n",
        "        \\right|\n",
        "        \\geq c\\frac{\\sigma}{\\sqrt{n\\delta}}\n",
        "      \\right)\n",
        "  \\right\\} \\geq \\delta.\n",
        "$$\n",
        "Explain why the above inequality implies that the sample mean estimator is not $L$-sub-Gaussian (for the family of distributions $\\mathcal{P}_{\\sigma^{2}}$) for any constant $L > 0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAUNoZSv20XU"
      },
      "source": [
        "class BadDistribution(Distribution):\n",
        "  \"\"\" A distributino on which the sample mean estimator is not sub-Gaussian. \"\"\"\n",
        "\n",
        "  def __init__(self, sigma, n, delta):\n",
        "    \"\"\" This distribution is parameterized by the sample size n and the\n",
        "    confidence level delta. \"\"\"\n",
        "    super().__init__(mu=0, sigma=sigma)\n",
        "    self.n = n\n",
        "    self.delta = delta\n",
        "\n",
        "  def generate_samples(self, n):\n",
        "    ############################################################################\n",
        "    # Exercise 3.2. Implement the code generating samples from a distribution\n",
        "    # that is unfavorable for the sample mean estimator.\n",
        "\n",
        "    ############################################################################\n",
        "\n",
        "# Executing the below block of code, you should see multiple points outside of\n",
        "# the sqrt{2}-sub-Gaussian condifence interval marked by the purple lines.\n",
        "# If, on the other hand, all of the observed samples concentrate within the\n",
        "# sub-Gaussian interval marked by the purple lines, your implementation of\n",
        "# BadDistribution class fails to visually establish that the sample mean\n",
        "# estimator is not \\sqrt{2}-sub-Gaussian.\n",
        "n = 100\n",
        "delta = 1e-3\n",
        "verify_subgaussianity(\n",
        "    estimator = SampleMean(),\n",
        "    distribution = BadDistribution(sigma=1.0, n=n, delta=delta),\n",
        "    delta = delta,\n",
        "    L = np.sqrt(2),\n",
        "    n=n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6LGImtT202i"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn6x2Gfy22cJ"
      },
      "source": [
        "**Exercise 3.1.** By Chebyshev's inequality, the following holds for any distribution $P \\in \\mathcal{P}_{\\sigma^{2}}$ and any $t > 0$:\n",
        "$$\n",
        "  \\mathbb{P}_{Z_{1}^{n} \\sim P^{n}}\\left(\n",
        "    \\left|\n",
        "      A^{(ERM)} - \\mathbf{E}_{Z \\sim P}[Z]\n",
        "    \\right|\n",
        "    \\geq t\n",
        "  \\right)\n",
        "  \\leq \\frac{\\mathrm{Var}[Z]}{n t^{2}}\n",
        "  \\leq \\frac{\\sigma^{2}}{n t^{2}}.\n",
        "$$\n",
        "Setting $t = t_{\\delta} = \\frac{\\sigma}{\\sqrt{n\\delta}}$ yields\n",
        "$$\n",
        "  \\mathbb{P}_{Z_{1}^{n} \\sim P^{n}}\\left(\n",
        "    \\left|\n",
        "      A^{(ERM)} - \\mathbf{E}_{Z \\sim P}[Z]\n",
        "    \\right|\n",
        "    \\geq \\frac{\\sigma}{\\sqrt{n \\delta}}\n",
        "  \\right)\n",
        "  \\leq \\delta.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJCZPUS1aq1W"
      },
      "source": [
        "**Exercise 3.2.** The below code block provides a sample implementation of a \"bad\" distribution for which the sample mean estimator fails to be sub-Gaussian.\n",
        "The distribution implemented in this exercise is further discussed in the solution of Exercise 3.3.\n",
        "\n",
        "The `generate_samples` function can be implemented as follows:\n",
        "```\n",
        "  def generate_samples(self, n):\n",
        "    # See the solution of Exercise 3.3 for the details of this distribution.\n",
        "    delta_prime = 20.0 * self.delta\n",
        "    p = delta_prime/self.n\n",
        "    Z = np.random.binomial(n=1, p=p, size=(n,))\n",
        "    signs = np.random.binomial(n=1, p=0.5, size=(n,))*2.0-1.0\n",
        "    Z = Z * signs\n",
        "    Z = Z * np.sqrt(self.n / delta_prime) * (self.sigma / np.sqrt(2.0))\n",
        "    # We add an independent random normal variable for a visual effect only.\n",
        "    # Note that we have rescaled the variance by 1/np.sqrt(2.0) so that the\n",
        "    # resulting random variable still has variance sigma^2.\n",
        "    Z += np.random.normal(loc=0.0, scale=(self.sigma / np.sqrt(2.0)), size=(n,))\n",
        "    return Z\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t7MOF960zAb"
      },
      "source": [
        "**Exercise 3.3.** We will show that the upper bound shown to hold via an application of Chebyshev's inequality (see solution to Exercise 3.1) is tight up to some multiplicative absolute constant $c > 0$.\n",
        "\n",
        "Given any sample size $n$ and any condifence level $\\delta \\in (0,1)$ define\n",
        "the distribution\n",
        "$$\n",
        "  P_{n,\\delta} = \\begin{cases}\n",
        "    0 & \\text{with probability }1 - \\frac{\\delta'}{n},\\\\\n",
        "    \\sigma\\sqrt{\\frac{n}{\\delta'}} & \\text{with probability } \\frac{\\delta'}{2n},\\\\\n",
        "    -\\sigma\\sqrt{\\frac{n}{\\delta'}} & \\text{with probability } \\frac{\\delta'}{2n}.\n",
        "  \\end{cases}\n",
        "$$\n",
        "for some $\\delta'$ to be specified later.\n",
        "\n",
        "Then a random variable $Z \\sim P_{n, \\delta}$ satisfies $\\mathbf{E}[Z] = 0$ and $\\mathrm{Var}[Z] = \\mathbf{E}[Z^{2}] = \\sigma^{2}$. In particular, $P_{n,\\delta} \\in \\mathcal{P}_{\\sigma^{2}}$. \n",
        "\n",
        "Now notice that\n",
        "\\begin{align*}\n",
        "  \\mathbf{P}_{Z_{1}^{n} \\sim P_{n,\\delta}}\\left(\n",
        "    \\left|\n",
        "      A^{(ERM)}\n",
        "    \\right|\n",
        "    \\geq \\frac{\\sigma}{\\sqrt{\\delta'n}}\n",
        "  \\right)\n",
        "  &\\geq\n",
        "  \\mathbf{P}_{Z_{1}^{n}}\\left(\n",
        "    \\left|\n",
        "      \\left\\{\n",
        "        i \\in \\{1, \\dots, n\\} : Z_{i} \\neq 0\n",
        "      \\right\\}\n",
        "    \\right|\n",
        "    = 1\n",
        "  \\right)\n",
        "  \\\\\n",
        "  &= n \\cdot \\frac{\\delta'}{n}\n",
        "  \\cdot\n",
        "  \\left(1 - \\frac{\\delta'}{n}\\right)^{n-1}\n",
        "\\end{align*}\n",
        "Now note that\n",
        "$$\n",
        "  \\lim_{n\\to\\infty}\\left(1 - \\frac{\\delta'}{n}\\right)^{n-1} = e^{-\\delta} \\geq e^{-1}.\n",
        "$$\n",
        "Thus for large enough $n$ we have\n",
        "$$\n",
        "  \\mathbf{P}_{Z_{1}^{n} \\sim P_{n,\\delta}}\\left(\n",
        "    \\left|\n",
        "      A^{(ERM)}\n",
        "    \\right|\n",
        "    \\geq \\frac{\\sigma}{\\sqrt{\\delta'n}}\n",
        "  \\right)\n",
        "  \\geq \\frac{e^{-1}}{2} \\delta'.\n",
        "$$\n",
        "Setting $\\delta' = 2e\\delta$ and $c = 1/\\sqrt{2e}$ finishes our proof.\n",
        "\n",
        "Now to see that the above construction rules out the possibility that the sample mean estimator is $L$-sub-Gaussian, it is enough to notice that for any $L > 0$ one can choose some small enough $\\delta \\in (0,1)$ such that\n",
        "$$\n",
        "  L\\sigma\\sqrt{\\log(2/\\delta)} \\ll c\\frac{\\sigma}{\\sqrt{\\delta}}.\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0rBcBNkF-hb"
      },
      "source": [
        "## Catoni's Estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f7ZagB0GDNa"
      },
      "source": [
        "In the previous section we have established that the sample mean estimator is not $L$-sub-Gaussian for any constant $L > 0$. We will now present the idea based on influence functions due to Olivier Catoni that leads to an L-sub-Gaussian estimator $A^{(Catoni)}_{\\sigma, \\delta}$ with $L = \\sqrt{2} + o(1)$. The estimator $A^{(Catoni)}$ is parametrized by the desired confidence level $\\delta$ and the oracle knowledge of the standard deviation $\\sigma$ of the unkown distribution $P \\in \\mathcal{P}_{\\sigma^{2}}$. Removing the dependence on $\\sigma$ is possible via Lepski's adaptation strategy (see the bibliographic remarks at the end of this notebook) at the expense of slightly worse bounds. On the other hand, the dependence on the desired confidence level $\\delta$ is known to be unavoidable in order to obtain a sub-Gaussian estimator with respect to the class $\\mathcal{P}_{\\sigma^{2}}$. \n",
        "\n",
        "<font color='green'>**Before diggint into the details, we remark that the tools used to derive a nearly optimal mean estimator are rather basic - one only needs to know Markov's inequality. It is remarkable that a creative application of such a simple technique allows us to obtain such a strong result.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGbZmkvoWc96"
      },
      "source": [
        "Before we go into the details of Catoni's estimator, let us slightly rewrite the proof of Exercise 2, where we have demonstrated that the sample mean estimator $A^{(ERM)}$ is $\\sqrt{2}$-sub-Gaussian for $\\sigma^{2}$-sub-Gaussian random variables. First, notice that the estimator $A^{(ERM)}$ can be written as\n",
        "$$\n",
        "  A^{(ERM)} \\in \\{a \\in \\mathbb{R} : S_{n}(a) = 0\\}\n",
        "  \\quad\\text{where}\\quad\n",
        "  S_{n}(a) = \\frac{1}{n}\\sum_{i=1}^{n}(Z_{i} - a). \\tag{3}\n",
        "$$\n",
        "Denote $\\mu = \\mathbf{E}[Z]$. Notice that the function $S_{n}(\\cdot)$ is non-increasing. Thus, in order to show that $|A^{(ERM)} - \\mu| \\leq t_{\\delta}$ with probability $1-\\delta$, it is enough to find $t_{\\delta}$ such that the following two inequalities hold simultaneously:\n",
        "\\begin{align*}\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left(S_{n}(\\mu + t_{\\delta}) > 0 \\right)\n",
        "  &\\leq \\delta/2, \\\\\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left(S_{n}(\\mu - t_{\\delta}) < 0 \\right)\n",
        "  &\\leq \\delta/2.\n",
        "\\end{align*}\n",
        "In the special case when the random variable $Z$ is $\\sigma^{2}$-sub-Gaussian, we have\n",
        "\\begin{align*}\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left(S_{n}(\\mu + t_{\\delta}) > 0 \\right)\n",
        "  &\\leq \\inf_{\\lambda > 0} \\left\\{\n",
        "    \\mathbf{E}_{Z_{1}^{n}}[\n",
        "    \\exp(\\lambda S_{n}(\\mu + t_{\\delta}))]\n",
        "    \\right\\}\n",
        "  \\\\\n",
        "  &= \\inf_{\\lambda > 0}\\left\\{\n",
        "     \\exp(-\\lambda t_{\\delta})\n",
        "    \\prod_{i=1}^{n}\\mathbf{E}_{Z_{i}}\\left[\n",
        "    \\exp\\left(\\frac{\\lambda}{n}(Z_{i} - \\mu)\\right)\n",
        "    \\right]\n",
        "  \\right\\}\n",
        "  \\\\\n",
        "  &\\leq \\inf_{\\lambda > 0} \\left\\{\n",
        "    \\exp\\left(\n",
        "    -\\lambda t_{\\delta} + \\frac{\\lambda^{2}\\sigma^{2}}{2n}\n",
        "    \\right)\n",
        "   \\right\\}\n",
        "  &\\text{since Z is }\\sigma^{2}\\text{-sub-Gaussian}\n",
        "  \\\\\n",
        "  &= \\exp\\left(\n",
        "    -\\frac{nt_{\\delta}^{2}}{2\\sigma^{2}}\n",
        "    \\right)\n",
        "  &\\text{by setting }\\lambda = \\frac{nt_{\\delta}}{\\sigma^{2}}.\n",
        "\\end{align*}\n",
        "In an analogous way, we also have\n",
        "\\begin{align*}\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left(S_{n}(\\mu - t_{\\delta}) < 0 \\right)\n",
        "  &=\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left(-S_{n}(\\mu - t_{\\delta}) > 0 \\right)\n",
        "  \\\\\n",
        "  &\\leq \\inf_{\\lambda > 0} \\left\\{\n",
        "    \\mathbf{E}_{Z_{1}^{n}}[\n",
        "    \\exp(-\\lambda S_{n}(\\mu - t_{\\delta}))]\n",
        "    \\right\\}\n",
        "  \\\\\n",
        "  &= \\inf_{\\lambda < 0} \\left\\{\n",
        "    \\mathbf{E}_{Z_{1}^{n}}[\n",
        "    \\exp(\\lambda S_{n}(\\mu - t_{\\delta}))]\n",
        "    \\right\\}\n",
        "  \\\\\n",
        "  &= \\inf_{\\lambda < 0}\\left\\{\n",
        "     \\exp(\\lambda t_{\\delta})\n",
        "    \\prod_{i=1}^{n}\\mathbf{E}_{Z_{i}}\\left[\n",
        "    \\exp\\left(\\frac{\\lambda}{n}(Z_{i} - \\mu)\\right)\n",
        "    \\right]\n",
        "  \\right\\}\n",
        "  \\\\\n",
        "  &\\leq \\inf_{\\lambda < 0} \\left\\{\n",
        "    \\exp\\left(\n",
        "    \\lambda t_{\\delta} + \\frac{\\lambda^{2}\\sigma^{2}}{2n}\n",
        "    \\right)\n",
        "   \\right\\}\n",
        "  &\\text{since Z is }\\sigma^{2}\\text{-sub-Gaussian}\n",
        "  \\\\\n",
        "  &= \\exp\\left(\n",
        "    -\\frac{nt_{\\delta}^{2}}{2\\sigma^{2}}\n",
        "    \\right)\n",
        "  &\\text{by setting }\\lambda = -\\frac{nt_{\\delta}}{\\sigma^{2}}.\n",
        "\\end{align*}\n",
        "The choice $t_{\\delta} = \\sqrt{2 \\sigma^{2} \\log(2/\\delta) / n}$ gives the desired result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw9COlUfmk4N"
      },
      "source": [
        "The crucial step in the derivations above is, of course, the use of sub-Gaussianity of the random variable $Z$ that allows us to upper bound the moment generating function $\\mathbf{E}[e^{\\lambda(Z - \\mu)}]$. <font color='green'>**In general, a random variable $Z$ distributed according to some $P \\in \\mathcal{P}_{\\sigma^{2}}$ need not have a well-defined moment generating function and the approach outlined above fails (as we have seen in Exercise 3, the sample mean estimator is not sub-Gaussian).**</font>\n",
        "\n",
        "The idea of Catoni's estimator is to replace the definition of $A^{(ERM)}$ stated in equation $(3)$ by\n",
        "$$\n",
        "  A^{(Catoni)}_{\\sigma, \\delta} \\in \\{a \\in \\mathbb{R} : S^{\\psi}_{n}(a) = 0\\}\n",
        "  \\quad\\text{where}\\quad\n",
        "  S_{n}^{\\psi}(a) = \\frac{1}{n}\\sum_{i=1}^{n}\\psi(Z_{i} - a),\n",
        "$$\n",
        "where $\\psi$ is some suitably chosen *non-decreasing influence function*.\n",
        "Notice that the choice $\\psi(x) = x$ recovers the sample mean estimator. On the other hand, notice that the choice of $\\psi(x)$ that grows much slower than $x$ would yield decreased influence of outliers arising through the potential heavy-tailedness of the unkown distribution of the random variable $Z$.\n",
        "\n",
        "It remains to find an appropriate candidate function $\\psi : \\mathbb{R} \\to \\mathbb{R}$ that yields an $L$-sub-Gaussian estimator of the mean with respect to the class of distributions $\\mathcal{P}_{\\sigma^{2}}$.\n",
        "We will attempt to find such $\\mathcal{\\psi}$ by working backwards in the derivations for the sample mean estimator presented above. \n",
        "\n",
        "Indeed, we want to find $\\psi$ such that for some $\\lambda > 0$ we have\n",
        "\\begin{align*}\n",
        "      &\\exp\\left(\n",
        "        -\\lambda t_{\\delta} + \\frac{\\lambda^{2}\\sigma^{2}}{2n}\n",
        "      \\right)\n",
        "  \\\\\n",
        "  &\\geq\n",
        "      \\exp\\left(\n",
        "        -\\lambda t_{\\delta} + \\frac{1}{2}\\frac{\\lambda^{2}}{n^{2}}\\sum_{i=1}^{n}\n",
        "        \\mathbf{E}_{Z_{i}}[(Z_{i} - \\mu)^{2}]\n",
        "      \\right)\n",
        "  &\\text{since for }P\\in \\mathcal{P}_{\\sigma^{2}} \\text{ we have }\\mathrm{Var}_{Z \\sim P}[Z] \\leq \\sigma^{2}\n",
        "  \\\\\n",
        "  &=\n",
        "      \\prod_{i=1}^{n}\n",
        "      \\exp\\left(\n",
        "        \\mathbf{E}_{Z_{i}}\\left[\n",
        "            \\frac{\\lambda}{n}(Z_{i} - (\\mu + t_{\\delta}))\n",
        "            + \\frac{1}{2}\\frac{\\lambda^{2}}{n^{2}}(Z_{i} - \\mu)^{2}\n",
        "        \\right]\n",
        "      \\right)\n",
        "  \\\\\n",
        "  &=\n",
        "      \\prod_{i=1}^{n}\n",
        "      \\exp\\left(\n",
        "        \\mathbf{E}_{Z_{i}}\\left[\n",
        "            \\frac{\\lambda}{n}(Z_{i} - (\\mu + t_{\\delta}))\n",
        "            + \\frac{1}{2}\\frac{\\lambda^{2}}{n^{2}}\\left(\n",
        "              Z_{i} - (\\mu + t_{\\delta})\n",
        "            \\right)^{2}\n",
        "        \\right]\n",
        "        - \\frac{1}{2}\\frac{\\lambda^{2}}{n^{2}}t_{\\delta}^{2}\n",
        "      \\right)\n",
        "  &\\text{since }\\mathbf{E}_{Z_{i}}[(X_{i} - \\mu)^{2}] = \\mathbf{E}_{Z_{i}}[(X_{i} - (\\mu + t_{\\delta}))^{2}] - t_{\\delta}^{2}.\n",
        "\\end{align*}\n",
        "Since we want to  rewrite the right hand side as a function of $S_{n}^{\\psi}(\\mu + t_{\\delta})$, let us multiply both sides by $\\exp(\\lambda^{2}t_{\\delta}^{2}/n)$. This yields, for any $\\lambda > 0$:\n",
        "\\begin{align*}\n",
        "    &\\exp\\left(\n",
        "      -\\lambda t_{\\delta} + \\frac{\\lambda^{2}(\\sigma^{2} + t_{\\delta}^{2})}{2n}\n",
        "    \\right)\n",
        "    \\\\\n",
        "    &\\geq\n",
        "    \\prod_{i=1}^{n}\n",
        "    \\exp\\left(\n",
        "      \\mathbf{E}_{Z_{i}}\\left[\n",
        "          \\frac{\\lambda}{n}(Z_{i} - (\\mu + t_{\\delta}))\n",
        "          + \\frac{1}{2}\\frac{\\lambda^{2}}{n^{2}}\\left(\n",
        "            Z_{i} - (\\mu + t_{\\delta})\n",
        "          \\right)^{2}\n",
        "      \\right]\n",
        "    \\right)\n",
        "    \\\\\n",
        "    &\\geq \\text{(... Exercise 4 ...)} \\tag{4}\n",
        "    \\\\\n",
        "    &= \\mathbf{E}_{Z_{1}^{n}}[\n",
        "      \\exp\\left(S_{n}^{\\psi}(\\mu + t_{\\delta})\\right)]\n",
        "    \\\\\n",
        "    &\\geq\n",
        "    \\mathbb{P}_{Z_{1}^{n}}\\left(S_{n}^{\\psi}(\\mu + t_{\\delta}) > 0\\right).\n",
        "\\end{align*}\n",
        "In a similar manner, for any $\\lambda > 0$ we have:\n",
        "\\begin{align*}\n",
        "    &\\exp\\left(\n",
        "      -\\lambda t_{\\delta} + \\frac{\\lambda^{2}(\\sigma^{2} + t_{\\delta}^{2})}{2n}\n",
        "    \\right)\n",
        "    \\\\\n",
        "    &\\geq\n",
        "    \\prod_{i=1}^{n}\n",
        "    \\exp\\left(\n",
        "      \\mathbf{E}_{Z_{i}}\\left[\n",
        "          -\\frac{\\lambda}{n}(Z_{i} - (\\mu - t_{\\delta}))\n",
        "          + \\frac{1}{2}\\frac{\\lambda^{2}}{n^{2}}\\left(\n",
        "            Z_{i} - (\\mu - t_{\\delta})\n",
        "          \\right)^{2}\n",
        "      \\right]\n",
        "    \\right)\n",
        "    \\\\\n",
        "    &\\geq \\text{(... Exercise 4 ...)} \\tag{5}\n",
        "    \\\\\n",
        "    &= \\mathbf{E}_{Z_{1}^{n}}[\n",
        "      \\exp\\left(-S_{n}^{\\psi}(\\mu - t_{\\delta})\\right)]\n",
        "    \\\\\n",
        "    &\\geq\n",
        "    \\mathbb{P}_{Z_{1}^{n}}\\left(-S_{n}^{\\psi}(\\mu - t_{\\delta}) > 0\\right).\n",
        "    \\\\\n",
        "    &=\n",
        "    \\mathbb{P}_{Z_{1}^{n}}\\left(S_{n}^{\\psi}(\\mu - t_{\\delta}) < 0\\right).\n",
        "\\end{align*}\n",
        "We now turn to Exercise 4, where we are asked to fill in the missing details in equations $(4)$ and $(5)$ above by finding the desired influence function $\\psi$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6f5UggBhci5"
      },
      "source": [
        "### Exercise 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CS-IYEG3sey"
      },
      "source": [
        "Find a non-decreasing influence function $\\psi = \\psi_{n, \\sigma, \\delta}$ (i.e., your choice if $\\psi$ is allowed to depend on $n, \\sigma$ and $\\delta$) that simultaneously satisfies the inequalities $(4)$ and $(5)$. Show that your choice of $\\psi$ yields an estimator $A^{(Catoni)}_{\\sigma, \\delta}$ defined in $(3)$ that satisfies\n",
        "$$\n",
        "    \\sup_{P \\in \\mathcal{P}_{\\sigma^{2}}}\n",
        "    \\left\\{\n",
        "      \\mathbb{P}_{Z_{1}^{n}}\\left(\n",
        "        \\left|\n",
        "          A^{(Catoni)}_{\\sigma, \\delta} - \\mathbf{E}_{Z \\sim P}[Z]\n",
        "        \\right|\n",
        "        \\geq\n",
        "        \\sqrt{\\frac{\n",
        "          2\\sigma^{2}\\log(2/\\delta)\n",
        "        }{\n",
        "          n\\left(1-\\frac{2\\log(2/\\delta)}{n}\\right)\n",
        "        }}\n",
        "      \\right)\n",
        "    \\right\\}\n",
        "    \\leq \\delta.\n",
        "$$\n",
        "Notice that modulo the multiplicative term\n",
        "$$\\sqrt{\\frac{1}{\\left(1-\\frac{2\\log(2/\\delta)}{n}\\right)}} = o(1),$$\n",
        "the confidence intervals guaranteed by the Catoni's estimator are as good as the confidence intervals we have obtained for the sample mean estimator applied to estimate the mean of subgaussian distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrBUr5ldJ8Bp"
      },
      "source": [
        "#### Hint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UjGbTD7J-Aa"
      },
      "source": [
        "Applying the inequality $e^{x} \\geq 1 + x$ valid for all $x \\in \\mathbb{R}$ to equation $(4)$ we have\n",
        "\\begin{align*}\n",
        "    &\\prod_{i=1}^{n}\n",
        "    \\exp\\left(\n",
        "      \\mathbf{E}_{Z_{i}}\\left[\n",
        "          \\frac{\\lambda}{n}(Z_{i} - (\\mu + t_{\\delta}))\n",
        "          + \\frac{1}{2}\\frac{\\lambda^{2}}{n^{2}}\\left(\n",
        "            Z_{i} - (\\mu + t_{\\delta})\n",
        "          \\right)^{2}\n",
        "      \\right]\n",
        "    \\right)\n",
        "    \\\\\n",
        "    &\\geq\n",
        "    \\prod_{i=1}^{n}\n",
        "    \\mathbf{E}_{Z_{i}}\\left[\n",
        "      1 \n",
        "      +\n",
        "      \\frac{\\lambda}{n}(Z_{i} - (\\mu + t_{\\delta}))\n",
        "      + \\frac{1}{2}\\frac{\\lambda^{2}}{n^{2}}\\left(\n",
        "            Z_{i} - (\\mu + t_{\\delta})\n",
        "        \\right)^{2}\n",
        "    \\right]\n",
        "    \\\\\n",
        "    &= \\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "        \\prod_{i=1}^{n}\\left(\n",
        "      1 \n",
        "      +\n",
        "      \\frac{\\lambda}{n}(Z_{i} - (\\mu + t_{\\delta}))\n",
        "      + \\frac{1}{2}\\frac{\\lambda^{2}}{n^{2}}\\left(\n",
        "            Z_{i} - (\\mu + t_{\\delta})\n",
        "        \\right)^{2}\n",
        "        \\right)\n",
        "    \\right].\n",
        "\\end{align*}\n",
        "Similarly, applying the inequality $e^{x} \\geq 1 + x$ to equation $(5)$ we obtain\n",
        "\\begin{align*}\n",
        "    &\\prod_{i=1}^{n}\n",
        "    \\exp\\left(\n",
        "      \\mathbf{E}_{Z_{i}}\\left[\n",
        "          -\\frac{\\lambda}{n}(Z_{i} - (\\mu - t_{\\delta}))\n",
        "          + \\frac{1}{2}\\frac{\\lambda^{2}}{n^{2}}\\left(\n",
        "            Z_{i} - (\\mu - t_{\\delta})\n",
        "          \\right)^{2}\n",
        "      \\right]\n",
        "    \\right)\n",
        "    \\\\\n",
        "    &\\geq\n",
        "    \\prod_{i=1}^{n}\n",
        "    \\mathbf{E}_{Z_{i}}\\left[\n",
        "      1 \n",
        "      -\n",
        "      \\frac{\\lambda}{n}(Z_{i} - (\\mu - t_{\\delta}))\n",
        "      + \\frac{1}{2}\\frac{\\lambda^{2}}{n^{2}}\\left(\n",
        "            Z_{i} - (\\mu - t_{\\delta})\n",
        "        \\right)^{2}\n",
        "    \\right]\n",
        "    \\\\\n",
        "    &= \\mathbf{E}_{Z_{1}^{n}}\\left[\n",
        "        \\prod_{i=1}^{n}\\left(\n",
        "      1 \n",
        "      -\n",
        "      \\frac{\\lambda}{n}(Z_{i} - (\\mu - t_{\\delta}))\n",
        "      + \\frac{1}{2}\\frac{\\lambda^{2}}{n^{2}}\\left(\n",
        "            Z_{i} - (\\mu - t_{\\delta})\n",
        "        \\right)^{2}\n",
        "        \\right)\n",
        "    \\right].\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PS3EaXmhUGz"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX3u21LrJ0Ts"
      },
      "source": [
        "Carrying on from the derivations in the hint, it is enough for the desired influence function $\\psi$ to satisfy\n",
        "$$\n",
        "      1 \n",
        "      +\n",
        "      \\frac{\\lambda}{n}(Z_{i} - (\\mu + t_{\\delta}))\n",
        "      + \\frac{1}{2}\\frac{\\lambda^{2}}{n^{2}}\\left(\n",
        "            Z_{i} - (\\mu + t_{\\delta})\n",
        "        \\right)^{2}\n",
        "      = \\exp\\left(\\frac{1}{n}\\psi\\left(Z_{i} - (\\mu + t_{\\delta})\\right)\\right)\n",
        "$$\n",
        "and\n",
        "$$\n",
        "      1 \n",
        "      -\n",
        "      \\frac{\\lambda}{n}(Z_{i} - (\\mu - t_{\\delta}))\n",
        "      + \\frac{1}{2}\\frac{\\lambda^{2}}{n^{2}}\\left(\n",
        "            Z_{i} - (\\mu - t_{\\delta})\n",
        "        \\right)^{2}\n",
        "      = \\exp\\left(-\\frac{1}{n}\\psi\\left(Z_{i} - (\\mu - t_{\\delta})\\right)\\right)\n",
        "$$\n",
        "Hence, it is enough for $\\psi$ to simultaneously satisfy the following two identities for any $x \\in \\mathbb{R}$:\n",
        "$$\n",
        "  \\log\\left(1 + \\frac{\\lambda}{n}x + \\frac{\\lambda^{2}}{n^{2}}x^{2}\\right)\n",
        "  = \\frac{1}{n}\\psi(x)\n",
        "  \\quad\\text{and}\\quad\n",
        "  \\log\\left(1 - \\frac{\\lambda}{n}x + \\frac{\\lambda^{2}}{n^{2}}x^{2}\\right)\n",
        "  = -\\frac{1}{n}\\psi(x).\n",
        "$$\n",
        "The following choice fullfils the two conditions above:\n",
        "\\begin{align}\n",
        "  \\psi(x) = \\begin{cases}\n",
        "    n\\log\\left(1 + \\frac{\\lambda}{n}x + \\frac{\\lambda^{2}}{n^{2}}x^{2}\\right)\n",
        "    &\\text{for }x \\geq 0, \\\\\n",
        "    -n\\log\\left(1 - \\frac{\\lambda}{n}x + \\frac{\\lambda^{2}}{n^{2}}x^{2}\\right)\n",
        "    &\\text{for }x < 0.\n",
        "  \\end{cases}\n",
        "\\end{align}\n",
        "\n",
        "It remains to find the smallest value $t_{\\delta} > 0$ such that for some $\\lambda > 0$ the left hand sides of the chain of equations containing\n",
        "$(4)$ and $(5)$ equal to $\\delta/2$. It is thus enough to solve the equation\n",
        "$$\n",
        "  \\frac{\\delta}{2} = \\exp\\left(\n",
        "      -\\lambda t_{\\delta} + \\frac{\\lambda^{2}(\\sigma^{2} + t_{\\delta}^{2})}{2n}\n",
        "  \\right)\n",
        "$$\n",
        "The right hand side above is optimized with the choice\n",
        "$$\n",
        "  \\lambda = \\frac{nt_{\\delta}}{\\sigma^{2} + t_{\\delta}^{2}}.\n",
        "$$\n",
        "Hence,\n",
        "\\begin{align*}\n",
        "  &\\frac{\\delta}{2}\n",
        "  = \n",
        "  \\exp\\left(  \n",
        "    -\\frac{nt_{\\delta}^{2}}{2(\\sigma^{2} + t_{\\delta}^{2})}\n",
        "  \\right)\n",
        "  \\\\\n",
        "  \\implies&\n",
        "  \\frac{2\\log(2/\\delta)}{n} = \\frac{t_{\\delta}^{2}}{\\sigma^{2} + t_{\\delta}^{2}}\n",
        "  \\\\\n",
        "  \\implies&\n",
        "  \\sqrt{\\frac{2\\sigma^{2}\\log(2/\\delta)}{n\\left(1 - \\frac{2\\log(2/\\delta)}{n}\\right)}} = t_{\\delta},\n",
        "\\end{align*}\n",
        "which proves the desired confidence bound.\n",
        "\n",
        "Finally, plugging in the values of $\\lambda$ and $t_{\\delta}$ computed above\n",
        "yields the influence function $\\psi$ defined as:\n",
        "$$\n",
        "  \\psi(x) = \\begin{cases}\n",
        "    n\\log\\left(1 + \\alpha x + \\alpha^{2} x^{2}\\right)\n",
        "    &\\text{for }x \\geq 0, \\\\\n",
        "    -n\\log\\left(1 - \\alpha x + \\alpha^{2} x^{2}\\right)\n",
        "    &\\text{for }x < 0,\n",
        "  \\end{cases}\n",
        "$$\n",
        "where\n",
        "$$\n",
        "  \\alpha = \\frac{\\lambda}{n} = \\frac{t_{\\delta}}{\\sigma^{2} + t_\\delta^{2}}.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZC1yVd83wI7"
      },
      "source": [
        "### Exercise 5\n",
        "\n",
        "- Implement Catoni's estimator $A^{(Catoni)}_{\\sigma, \\delta}$ described in equation $(3)$ and the solution of Exercise 4.\n",
        "- Use the estimator to estimate the mean of a\n",
        "  - `GaussianDistribution`;\n",
        "  - `BadDistribution` implemented in Exercise 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LykOFjujad7d"
      },
      "source": [
        "class CatonisEstimator(Estimator):\n",
        "  \"\"\" An implementation of Catoni's estimator described in the solution of\n",
        "  Exercise 4. \"\"\"\n",
        "\n",
        "  def __init__(self, sigma, delta):\n",
        "    \"\"\" The estimator is parametrized by the oracle knowledge of the true\n",
        "    standard deviation sigma (or an upper bound on it) and the desired\n",
        "    confidence level delta. \"\"\"\n",
        "    self.sigma = sigma\n",
        "    self.delta = delta\n",
        "\n",
        "  def estimate_mean(self, Z):\n",
        "    ############################################################################\n",
        "    # Exercise 5. Implement Catoni's mean estimator.\n",
        "    \n",
        "    ############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC5oonVz3Z-Q"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKX1_sfQbHLe"
      },
      "source": [
        "Catoni's estimator can be implemented as follows:\n",
        "```\n",
        "class CatonisEstimator(Estimator):\n",
        "  \"\"\" An implementation of Catoni's estimator described in the solution of\n",
        "  Exercise 4. \"\"\"\n",
        "\n",
        "  def __init__(self, sigma, delta):\n",
        "    \"\"\" The estimator is parametrized by the oracle knowledge of the true\n",
        "    standard deviation sigma (or an upper bound on it) and the desired\n",
        "    confidence level delta. \"\"\"\n",
        "    self.sigma = sigma\n",
        "    self.delta = delta\n",
        "\n",
        "  def estimate_mean(self, Z):\n",
        "    n = len(Z)\n",
        "    t_delta = np.sqrt(2.0 * self.sigma**2 * np.log(2.0/self.delta))\n",
        "    t_delta /= np.sqrt(n - 2.0*np.log(2.0/self.delta))\n",
        "    alpha = t_delta / (self.sigma**2 + t_delta**2)\n",
        "    return self.solve(Z, alpha)\n",
        "\n",
        "  def psi(self, alpha, x):\n",
        "    signs = np.sign(x)\n",
        "    x_abs = x * signs\n",
        "    unsigned_answer = np.log(1.0 + alpha*x_abs + alpha**2 * x_abs**2 / 2.0)\n",
        "    return unsigned_answer * signs\n",
        "\n",
        "  def Sn(self, alpha, x):\n",
        "    return np.sum(self.psi(alpha, x))\n",
        "\n",
        "  def solve(self, Z, alpha, accuracy = 1e-6):\n",
        "    \"\"\" We will find an approximate solution to Sn(a) = 0 via binary search.\n",
        "    \"\"\"\n",
        "    left_endpoint = 0.0\n",
        "    right_endpoint = 0.0\n",
        "\n",
        "    Delta = 1.0\n",
        "    while self.Sn(alpha, Z - left_endpoint) < 0.0:\n",
        "      left_endpoint -= Delta\n",
        "      Delta *= 2.0\n",
        "\n",
        "    Delta = 1.0\n",
        "    while self.Sn(alpha, Z - right_endpoint) > 0.0:\n",
        "      right_endpoint += Delta\n",
        "      Delta *= 2.0\n",
        "\n",
        "    while right_endpoint - left_endpoint > accuracy:\n",
        "      midpoint = (right_endpoint + left_endpoint) / 2.0\n",
        "      if self.Sn(alpha, Z - midpoint) > 0.0:\n",
        "        left_endpoint = midpoint\n",
        "      else:\n",
        "        right_endpoint = midpoint\n",
        "\n",
        "    return (left_endpoint + right_endpoint) / 2.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFJvjqKkbrrQ"
      },
      "source": [
        "Below we apply the estimator to estimate the mean of Gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nG2EkQjMFpY"
      },
      "source": [
        "delta = 1e-3\n",
        "sigma = 1.0\n",
        "catonis_estimator = CatonisEstimator(sigma, delta)\n",
        "\n",
        "for estimator in [SampleMean(), catonis_estimator]:\n",
        "  confidence_interval = verify_subgaussianity(\n",
        "      estimator=estimator,\n",
        "      distribution=GaussianDistribution(mu=0.0, sigma=sigma),\n",
        "      delta=delta,\n",
        "      L = np.sqrt(2),\n",
        "      n = 100)\n",
        "  title = confidence_interval.ax.get_title()\n",
        "  confidence_interval.ax.set_title(title + ', estimator = ' + \\\n",
        "                                   str(type(estimator).__name__), size=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aY-suKuby42"
      },
      "source": [
        "Next, we apply the Catoni's distribution to estimate the mean of `BadDistribution` (i.e., the distribution for which the sample mean estimator fails to be sub-Gaussian)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIbJ6IS_WNre"
      },
      "source": [
        "delta = 1e-3\n",
        "sigma = 1.0\n",
        "n = 100\n",
        "catonis_estimator = CatonisEstimator(sigma, delta)\n",
        "bad_distribution = BadDistribution(sigma, n, delta)\n",
        "\n",
        "for estimator in [SampleMean(), catonis_estimator]:\n",
        "  confidence_interval = verify_subgaussianity(\n",
        "      estimator=estimator,\n",
        "      distribution=bad_distribution,\n",
        "      delta=delta,\n",
        "      L = np.sqrt(2),\n",
        "      n = n)\n",
        "  title = confidence_interval.ax.get_title()\n",
        "  confidence_interval.ax.set_title(title + ', estimator = ' + \\\n",
        "                                   str(type(estimator).__name__), size=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "459YSQi1GeDt"
      },
      "source": [
        "## Bibliographic Remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq3Y8I95YGTT"
      },
      "source": [
        "The study of statistical estimators robust to heavy-tailed distributions was initiated by *Catoni [2012]*, where sub-optimality of the sample mean estimator was established (cf. Exercise 3), and Catonis estimator of Exercise 4 was proposed; see the paper for extensions to variance estimation as well as removing the dependence on the unkown variance in the Catonis estimator. The dependence on the desired confidence level $\\delta$, however, cannot be removed by any estimator unless one is willing to make additional distributional assumptions; see *Devroye, Lerasle, Lugosi, and Oliveira [2016]* for details as well as other results concerning real-valued mean estimation robust to heavy-tailed distributions. For results concerning vector-valued mean estimation see, for example, *Minsker [2015]*, *Lugosi and Mendelson [2019a]*; for results concerning regression under heavy-tailed data see, for example, *Audibert and Catoni [2011]*, *Lugosi and\n",
        "Mendelson [2019b]*, *Lecu and Lerasle [2020]*, *Mourtada, Vakeviius, and Zhivotovskiy [2021]*. The list of above-cited references is far from being exhaustive. See the recent survey by *Lugosi and Mendelson [2019c]* for further references and an excellent introduction to the subject.\n",
        "\n",
        "**References**\n",
        "\n",
        "J.-Y. Audibert and O. Catoni. Robust linear least squares regression. The Annals of Statistics, 39(5):27662794, 2011.\n",
        "\n",
        "\n",
        "O. Catoni. Challenging the empirical mean and empirical variance: a deviation study. In Annales de lIHP Probabilits et statistiques, volume 48, pages 11481185, 2012.\n",
        "\n",
        "\n",
        "L. Devroye, M. Lerasle, G. Lugosi, and R. I. Oliveira. Sub-gaussian mean estimators. Annals of Statistics, 44(6):26952725, 2016.\n",
        "\n",
        "\n",
        "G. Lecu and M. Lerasle. Robust machine learning by median-of-means: theory and practice. Annals of Statistics, 48(2):906931, 2020.\n",
        "\n",
        "\n",
        "G. Lugosi and S. Mendelson. Mean estimation and regression under heavy-tailed\n",
        "distributionsa survey. arXiv preprint arXiv:1906.04280, 2019a.\n",
        "\n",
        "\n",
        "G. Lugosi and S. Mendelson. Risk minimization by median-of-means tournaments. Journal of the European Mathematical Society, 22(3):925965, 2019b.\n",
        "\n",
        "\n",
        "G. Lugosi and S. Mendelson. Sub-gaussian estimators of the mean of a random vector. The Annals of Statistics, 47(2):783794, 2019c.\n",
        "\n",
        "\n",
        "S. Minsker. Geometric median and robust estimation in banach spaces. Bernoulli, 21(4): 23082335, 2015.\n",
        "\n",
        "\n",
        "J. Mourtada, T. Vakeviius, and N. Zhivotovskiy. Distribution-free robust linear regression. arXiv preprint arXiv:2102.12919, 2021.\n",
        "\n"
      ]
    }
  ]
}