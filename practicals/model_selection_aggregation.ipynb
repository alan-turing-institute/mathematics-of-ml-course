{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_selection_aggregation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2u1rA-Ybi5Qo",
        "GyXMuWWnjtrJ",
        "hZov5IZyLPN2"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHWECt_9yhiQ"
      },
      "source": [
        "# Model Selection Aggregation\n",
        "\n",
        "<font color='green'>In this practical session, we take a look at the problem of model selection aggregation, where a statistical learning algorithm aims to select a function that performs as well as the best function in some finite set.\n",
        "The finite nature of this problem will allow us to introduce many statistical phenomena of interest without going too deep into various technicalities present for problems involving an infinite number of candidate functions.\n",
        "Our main objectives are the following:\n",
        "</font>\n",
        "- <font color='green'>becoming familiar with the notion of minimax optimality;</font>\n",
        "- <font color='green'>understanding the difference between proper and improper learning;</font>\n",
        "- <font color='green'>introducing a problem instance that is hard for information-theoretic reasons;</font>\n",
        "- <font color='green'>using the hard problem instance to establish suboptimality of proper learning procedures;</font>\n",
        "- <font color='green'>showing that an improper procedure based on exponential weighting is \"on average\" optimal;</font>\n",
        "- <font color='green'>introducing a learning procedure that guarantees a \"high-probability\" optimality, which is a stronger notion than being optimal \"on average\".</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW_ImBNuNNZY"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this practical session we consider the problem of *model selection aggregation* comprised of a finite dictionary of functions $\\mathcal{A} = \\{a_{i} : \\mathcal{X} \\to \\mathcal{Y} : i = 1, \\dots, m\\}$ and $n$ data points $Z_{i} = (X_{i}, Y_{i}) \\in \\mathcal{X} \\times \\mathcal{Y} \\subseteq \\mathbb{R}^{d} \\times \\mathbb{R}$ sampled i.i.d. from some unkown distribution $P$. An aggregation procedure aims to output an *aggregate* function $A = A(Z_{1}, \\dots, Z_{n}) : \\mathbb{R}^{d} \\to \\mathcal{Y}$ that performs as well as the best function in $\\mathcal{A}$ in the sense described below. We will restrict our attention to aggregation procedures constrained to produce convex combinations of elements of $\\mathcal{A}$. That is, each function $A = A(Z_{1}, \\dots, Z_{n})$ can be uniquely identified by a vector\n",
        "$W = W(Z_{1}, \\dots, Z_{n}) \\in \\mathbb{R}^{m}$ such that\n",
        "\\begin{equation}\n",
        "  A = \\sum_{i=1}^{m} W_{i}a_{i} = a_{W}\n",
        "  \\quad\\text{such that}\\quad\n",
        "  \\sum_{i=1}^{m} W_{i} = 1 \\text{ and }\n",
        "  W_{i} \\geq 0 \\text{ for all }i=1,\\dots,m.\n",
        "\\end{equation}\n",
        "\n",
        "In what follows, we identify the output of an aggregation procedure by a vector $W = W(Z_{1}, \\dots, Z_{n})$. In addition, we identify each function $a_{i} \\in \\mathcal{A}$ via a standard basis vector $e_{i} \\in \\mathbb{R}^{m}$ (i.e., a vector whose $i$-th element equals $1$ while all the other elements equal $0$). In particular, $a_{i} = a_{e_{i}}$.\n",
        "\n",
        "## Different Measures of Quality\n",
        "\n",
        "In this practical session we only consider the quadratic loss function $\\ell(a, Z) = (a(X) - Y)^{2}$. Recall that the aim of an aggregate $W$ is to predict as well as the best function in $\\mathcal{A}$. Henceforth, the quality of an aggregation procedure $W$ is measured by its *estimation error* $\\mathcal{E}_{n, \\mathcal{A}, P}(W)$ defined as\n",
        "\\begin{align}\n",
        "  \\mathcal{E}_{n,P,\\mathcal{A}}(W)\n",
        "  &=\n",
        "  r(a_{W}) - \\min_{a \\in \\mathcal{A}} r(a)\n",
        "  \\\\\n",
        "  &=\n",
        "  \\mathbf{E}_{Z \\sim P}[(a_{W}(X) - Y)^{2} \\vert Z_{1}, \\dots, Z_{n}]\n",
        "  -  \\min_{a \\in \\mathcal{A}} \\mathbf{E}_{Z \\sim P}[(a(X) - Y)^{2}].\n",
        "\\end{align}\n",
        "When $n$, $P$ and $\\mathcal{A}$ are clear from the context, we will use the shorthand notation $\\mathcal{E}(W) = \\mathcal{E}_{n,P,\\mathcal{A}}$.\n",
        "\n",
        "We will study two different measures of quality:\n",
        "\n",
        "- *In-expectation performance.* The first measure of quality concerns understanding *average* performance of an estimator $W$: given an aggregation procedure $W$ we want to obtain upper bounds $\\Delta_{n,m}^{\\mathrm{EX}}$ on its expected estimation error\n",
        "\\begin{equation}\n",
        "  \\mathbf{E}_{Z_{1}^{n}}[\\mathcal{E}_{n,P,\\mathcal{A}}(W)] \\leq \\Delta_{n,m}^{\\mathrm{EX}}.\n",
        "\\end{equation}\n",
        "where $Z_{1}^{n}$ denotes the observed sample $Z_{1}, \\dots, Z_{n}$.\n",
        "Note that the terms $\\Delta_{n,m}^{\\mathrm{EX}}$ should only depend on the sample size $n$ and the dictionary size $m$. In particular, we want to obtain performance guarantees that hold for any distribution $P$ and any dictionary of the given size $m$.\n",
        "\n",
        "- *In-deviations performance*. The second measure of quality concerns understanding the tail behaviour of $\\mathcal{E}_{n,P,\\mathcal{A}}$. Given a confidence parameter $\\delta$ and an aggregation procedure $W$ we want to obtain upper bounds $\\Delta_{n,m,\\delta}^{\\mathrm{PR}}$ that satisfy the following inequality\n",
        "\\begin{equation}\n",
        "  \\tag{1}\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left( \\mathcal{E}_{n,P,\\mathcal{A}}(W) \\geq \\Delta_{n,m,\\delta}^{\\mathrm{PR}} \\right) \\leq \\delta.\n",
        "\\end{equation}\n",
        "\n",
        "## Minimax Optimality\n",
        "\n",
        "A sequence $(\\Delta^{\\mathrm{EX}}_{n,m})$ is called an (in-expectation) *minimax optimal rate* of model selection aggregation aggregation if the following two conditions hold:\n",
        "1. There exists an absolute constant $c_{1}$ such that for any dictionary size $m$ the following holds:\n",
        "\\begin{equation}\n",
        "  \\inf_{W} \\sup_{P, \\mathcal{A} : |\\mathcal{A}| = m} \\mathbf{E}_{Z_{1}^{n}}[\\mathcal{E}_{n,P,\\mathcal{A}}(W)]\n",
        "  \\geq c_{1}\n",
        "  \\Delta_{n,m}^{\\mathrm{EX}}.\n",
        "\\end{equation}\n",
        "  The above infimum is taken over all possible aggregation procedures while the above supremum is taken over all possible distributions $P$ supported on $\\mathcal{X} \\times \\mathcal{Y}$ and all dictionaries $\\mathcal{A}$ of size $m$ of functions mapping $\\mathcal{X}$ to $\\mathcal{Y}$.\n",
        "2. There exists an absolute constant $c_{2}$ and an aggregation procedure $W$ such that for any dictionary size $m$ and any distribution $P$ the following holds:\n",
        "\\begin{equation}\n",
        "  \\mathbf{E}_{Z_{1}^{n}}[\\mathcal{E}_{n,P,\\mathcal{A}}(W)] \\leq c_{2}\n",
        "  \\Delta_{n,m}^{\\mathrm{EX}}.\n",
        "\\end{equation}\n",
        "\n",
        "In the bounded setting with $\\mathcal{Y} = [-1, 1]$ the in-expectation minimax optimal rate is equal to \n",
        "\\begin{equation}\n",
        "  \\tag{2}\n",
        "  \\Delta_{n,M}^{\\mathrm{EX}} = \\frac{\\log m}{n}.\n",
        "\\end{equation}\n",
        "We will call any aggregation procedure $W$ that attains this rate *expectation-optimal*.\n",
        "\n",
        "Similarly, we say that an aggregation procedure $W$ is *deviation-optimal* if there exists some absolute constant $c$ such that $W$ satisfies $(1)$ with\n",
        "\\begin{equation}\n",
        "  \\tag{3}\n",
        "  \\Delta_{n,M,\\delta}^{\\mathrm{PR}} = c \\frac{\\log m + \\log(1/\\delta)}{n}.\n",
        "\\end{equation}\n",
        "\n",
        "<font color='green'>**In this practical session, we will aim to derive expectation-optimal and deviation-optimal procedures for the problem of model selection aggregation. In Exercise 1 you will be asked to prove that deviation optimality implies expectation optimality and hence it is a condition that is harder to satisfy. In what follows we will always assume that $\\mathcal{Y}=[-1, 1]$.**</font>\n",
        "\n",
        "## Proper and Improper Learning\n",
        "\n",
        "Notice that the performance measure $\\mathcal{E}(W)$ as defined by\n",
        "\\begin{equation}\n",
        "  \\mathcal{E}(W) = r(a_{W}) - \\min_{a \\in \\mathcal{A}} r(a)\n",
        "\\end{equation}\n",
        "does not exactly match the notion of estimation error introduced in the lectures. In the lectures, the estimation error was defined only for *proper* learning algorithms &mdash; algorithms that always output a function from the set $\\mathcal{A}$. Thus, for the model selection aggregation problem, an estimator $W$ is proper if and only if it always outputs one of the basis vectors $\\{e_{1}, \\dots, e_{m}\\}$. A canonical example of such an estimator is the empirical risk minimization algorithm that we will explore in the exercises below. In contrast to proper estimators, an estimator that is allowed to output a function outside of the reference class of functions $\\mathcal{A}$ is called *improper*.\n",
        "\n",
        "<font color='green'>**Improper estimators have recently been attracting more and more attention in the literature, since such procedures can offer both computational and statistical advantages. One of the goals of this practical session is to demonstrate a simple scenario where achieving minimax optimal rates is only possible via improper estimators.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5exEfOX7yV0a"
      },
      "source": [
        "### Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWYafUjLy0aU"
      },
      "source": [
        "In this exercise we show that satisfying the deviation-optimal rate $(3)$ is at least as difficult as sastisfying the expectation-optimal rate $(2)$.\n",
        "\n",
        "Suppose that some aggregation procedure $W$ satisfies the following for any dictionary $\\mathcal{A}$ of size $m \\geq 2$, any distribution $P$, and any confidence\n",
        "$\\delta \\in (0,1)$:\n",
        "\\begin{equation}\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left(\n",
        "    \\mathcal{E}_{n,P,\\mathcal{A}}(W) \\geq c_{1}\\frac{\\log m + \\log (1/\\delta)}{n} \\right) \\leq \\delta,\n",
        "\\end{equation}\n",
        "where $c_{1}$ is some absolute constant.\n",
        "\n",
        "Show that there exists an absolute constant $c_{2}$ such that the estimator $W$ satisfies the following in-expectation guarantee for any dictionary of size $m$ and any distribution $P$:\n",
        "\\begin{equation}\n",
        "  \\mathbf{E}_{Z_{1}^{n}}[\\mathcal{E}_{n,P,\\mathcal{A}}(W)] \\leq c_{2}\\frac{\\log m}{n}.\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSGrBoBO0XOJ"
      },
      "source": [
        "#### Hint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMFwusiD0crt"
      },
      "source": [
        "Let $X$ be a non-negative random variable. Then $\\mathbf{E}[X] = \\int_{0}^{\\infty} \\mathbb{P}(X > x)dx$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhxPi-cM09Aq"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF7-LMWb1AmG"
      },
      "source": [
        "Let $\\mathbb{1}_{A}$ denote the indicator function of the event $A = \\{\\mathcal{E}(W) - c_{1}\\frac{\\log m}{n} \\leq 0\\} $: i.e., $\\mathbb{1}_{A} = 1$ if the event $A$ is true and $0$ otherwise. Let $A^{c} = \\{ \\mathcal{E}(W) - c_{1}\\frac{\\log m}{n} > 0\\}$ denote the complement of the event $A$. Then \n",
        "\\begin{align}\n",
        "  \\mathbf{E}_{Z_{1}^{n}}\\left[\\mathcal{E}(W)\\right]\n",
        "  &= \\mathbf{E}_{Z_{1}^{n}}\\left[\\mathcal{E}(W)(\\mathbb{1}_{A} + \\mathbb{1}_{A^{c}})\\right]\n",
        "  \\\\\n",
        "  &= \\mathbf{E}_{Z_{1}^{n}}\\left[\\left(\\mathcal{E}(W) - c_{1}\\frac{\\log m}{n} + c_{1}\\frac{\\log m}{n}\\right)(\\mathbb{1}_{A} + \\mathbb{1}_{A^{c}})\\right]\n",
        "  \\\\\n",
        "  &= c_{1}\\frac{\\log m}{n} + \\mathbf{E}_{Z_{1}^{n}}\\left[\\left(\\mathcal{E}(W) - c_{1}\\frac{\\log m}{n}\\right)(\\mathbb{1}_{A} + \\mathbb{1}_{A^{c}})\\right]\n",
        "  \\\\\n",
        "  &\\leq c_{1}\\frac{\\log m}{n}\n",
        "    + \\mathbf{E}_{Z_{1}^{n}}\\left[\\left(\\mathcal{E}(W) - c_{1}\\frac{\\log{m}}{n}\\right)\\mathbb{1}_{A^{c}} \\right]\n",
        "  \\\\\n",
        "  &= c_{1}\\frac{\\log m}{n}\n",
        "    + \\int_{0}^{\\infty} \\mathbb{P}_{Z_{1}^{n}}\\left(\\mathcal{E}(W) - c_{1}\\frac{\\log{m}}{n} > x \\right)dx\n",
        "    &\\text{(by the hint)}\n",
        "  \\\\\n",
        "  &\\leq c_{1}\\frac{\\log m}{n}\n",
        "    + \\int_{0}^{\\infty} \\exp(-nx/c_{1})dx\n",
        "    &\\text{(by the assumption on the in-deviations performance of W)}\n",
        "  \\\\\n",
        "  &= c_{1}\\frac{\\log (m) + 1}{n}.\n",
        "\\end{align}\n",
        "Since $m \\geq 2$, we have $1 \\leq \\log(m)/\\log(2)$ and hence the result follows by taking $c_{2} = c_{1}(1 + 1/\\log(2))$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp1lGqs_COoJ"
      },
      "source": [
        "## Setting up the Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTzpoaH_C5Au"
      },
      "source": [
        "We now turn to experiments. First, let us import the necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8CUvDnCNMpo"
      },
      "source": [
        "import numpy as np # for manipulating arrays.\n",
        "from matplotlib import pyplot as plt # for plotting.\n",
        "from math import isclose # for checking if two floating point numbers are close.\n",
        "from tqdm.notebook import tqdm # for displaying progress bars.\n",
        "from sklearn.linear_model import Ridge # for fitting ridge regression estimator."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h34dIsVwD2H2"
      },
      "source": [
        "Verifying whether a given estimator $W$ attains the optimal (in-deviations or in-expectation) rate is the focal point of this practical session. Since we will want to test multiple estimators in multiple problem settings, it will be useful to introduce abstractions for an estimator, problem setting and for the  code used to estimate estimation error $\\mathcal{E}(W)$.\n",
        "\n",
        "We begin by introducing an abstraction for an estimator. We will implement particular subclasses later, once we begin to explore the possible procedures for model selection aggregation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKyoReVIHjyX"
      },
      "source": [
        "class Estimator(object):\n",
        "  \"\"\" A base class for model selection aggregation estimators. \"\"\"\n",
        "\n",
        "  def fit(self, problem):\n",
        "    \"\"\"\n",
        "      :problem: An object of type Problem (implemented in the next cell).\n",
        "      :returns: A fitted aggregate function a_{w} represented by a vector\n",
        "        w \\in \\R^{m} .\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"not implemented\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEiu7YFKHod6"
      },
      "source": [
        "Next, we introduce an abstraction for a class that represents a problem instance. A problem instance is composed of the sample size $n$, the data generating mechanism $P$ and the dictionary of functions $\\mathcal{A}$. The abstrat class `Problem` defined below implements some functionality that is common for all problem instances, such as computing the population risk of a function $a_{w}$ when given access to population risks of the dictionary elements (implemented via `self.r`) and given access to an oracle that knows how to compute $L_{2}$ squared norms of functions $a_{w'}$ (implemented via  `self.compute_L2_squared_norm(w')`). The $L_{2}$ squared norm of a function $a_{w'} = \\sum_{i=1}^{m}w'_{i}a_{i}$ defined as $\\|a_{w'}\\|_{L_2}^{2} = \\mathbf{E}_{Z}[a_{w'}(X)^{2}]$.\n",
        "\n",
        "You will notice that some of the methods defining the below class are not implemented. You will be asked to implement them in the next exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mje5OzJGNxvG"
      },
      "source": [
        "class Problem(object):\n",
        "  \"\"\" A base class for problem instances.\n",
        "\n",
        "  This abstract class provides functionality that is common to all problem\n",
        "  instances. Two methods, namely, _generate_data and compute_L2_squared_norm\n",
        "  should be implemented by particular subclasses, since these methods are\n",
        "  distribution-dependent.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, A, r, n):\n",
        "    \"\"\" :A: A dictionary of functions [a_1, ..., a_d]. Given a matrix X of size\n",
        "          n \\times d, a dictionary function a returns a vector y = a(X) \\in\n",
        "          \\mathbb{R}^{n}, whose i-th element is equal to applying the dictionary\n",
        "          function a to the i-th row of the matrix X.\n",
        "        :r: A list of population risks [r(a_1), ..., r(a_m)],\n",
        "        :n: Sample size.\n",
        "    \"\"\"\n",
        "    self.A = A\n",
        "    self.r = r\n",
        "    self.n = n\n",
        "    self.resample_data()\n",
        "\n",
        "  def resample_data(self):\n",
        "    \"\"\" This method resamples the observed data Z_{1}, \\dots, Z_{n} and stores\n",
        "    it as class members self.X and self.y. \"\"\"\n",
        "    self.X, self.y = self._generate_data()\n",
        "\n",
        "  def _generate_data(self):\n",
        "    \"\"\" Generates n i.i.d. data points Z_{1} = (X_{1}, Y_{1}), \\dots,\n",
        "    Z_{n} = (X_{n}, Y_{n}).\n",
        "    \n",
        "    :returns: The generated data represented as (X, y), where:\n",
        "      X is an n \\times d numpy array whose $i$-th row is equal to X_{i};\n",
        "      y is an n \\times 1 numpy array whose $i$-th element is equal to Y_{i}.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\n",
        "        \"This distribution-dependent method should be implemented by a \"\n",
        "        \"subclass.\")\n",
        "    \n",
        "  def compute_L2_squared_norm(self, w):\n",
        "    \"\"\" Given a parameter vector w \\in \\mathbb{R}^{m}, returns the L2 squared\n",
        "    norm of the function a_{w} = \\sum_{i=1}^{m} w_{i}a_{i}. Recall that the\n",
        "    L2 squared norm of a function a is given by ||a||_{L2}^{2} = E [a(X)^2]. \"\"\"\n",
        "    raise NotImplementedError(\n",
        "        \"This distribution-dependent method should be implemented by a \"\n",
        "        \"subclass.\")\n",
        "\n",
        "  def compute_empirical_risk(self, a):\n",
        "    \"\"\" Given a function a computes its empirical risk on the dataset\n",
        "    (self.X, self.y) stored in this class.\n",
        "\n",
        "    :a: A function mapping \\mathbb{R}^{d} to \\mathbb{R}. Given a matrix X of\n",
        "      size n \\times d, the function a should return a vector y = a(X) of size\n",
        "      n \\times 1, whose i-th element is equal to the value of the function a\n",
        "      applied to the i-th row of the matrix X.\n",
        "    :returns: The emprical risk of the function a defined as\n",
        "      R(a) = \\frac{1}{n}\\sum_{i=1}^{n} (a(X_{i}) - Y_{i})^{2}.\n",
        "    \"\"\"\n",
        "    ############################################################################\n",
        "    # Exercise 2.1 - fill in the code below.\n",
        "\n",
        "    ############################################################################\n",
        "\n",
        "  def compute_r(self, w):\n",
        "    \"\"\" :w: A convex combination of dictionary elements a_{w} =\n",
        "    \\sum_{i=1}^{m} w_{i}a_{i} represented by an m-dimensional vector w.\n",
        "        :returns: The population risk of the function a_w, that is,\n",
        "          r(a_w) = E[(a_{w}(X) - Y)^{2}].\n",
        "    \"\"\"\n",
        "\n",
        "    # Let us first check if the parameter vector w defines a convex combination.\n",
        "    w = np.array(w).flatten()\n",
        "    assert len(w) == len(self.A)\n",
        "    assert (w >= 0).all()\n",
        "    assert isclose(np.sum(w), 1.0, abs_tol=1e-8)\n",
        "\n",
        "    ############################################################################\n",
        "    # Exercise 2.2 - fill in the code below.\n",
        "    # Hint: make use of the abstract method self.compute_empirical_risk.\n",
        "    \n",
        "    ############################################################################\n",
        "\n",
        "  def sample_estimation_errors(self, estimator, n_seeds):\n",
        "    \"\"\" This method provides the functionality for generating samples of the\n",
        "    random variable \\mathcal{E}(W).\n",
        "      :estimator: An object of type Estimator (see the abstract class above)\n",
        "        that represents the estimator W.\n",
        "      :n_seeds: Number of different i.i.d. estimates of the estimation error\n",
        "        \\mathcal{E}(W).\n",
        "      :returns: An array of length n_seeds, whose elements contain i.i.d.\n",
        "        samples of the random variable \\mathcal{E}(W).\n",
        "    \"\"\"\n",
        "    estimation_errors = np.zeros(n_seeds)\n",
        "    min_r = np.min(self.r)\n",
        "    for seed_idx, seed in tqdm(enumerate(range(n_seeds)), total=n_seeds):\n",
        "      np.random.seed(seed)\n",
        "      self.resample_data()\n",
        "      w = estimator.fit(self)\n",
        "      estimation_errors[seed_idx] = self.compute_r(w) - min_r\n",
        "    return estimation_errors    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5X4grplIvEK"
      },
      "source": [
        "### Exercise 2\n",
        "Fill in the missing implementation details in the `Problem` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPA1J17pI41r"
      },
      "source": [
        "#### Hint for Exercise 2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_7_YGC3Sx2G"
      },
      "source": [
        "Make use of the following expression. For any two functions $a, a'$ and $\\lambda \\in [0, 1]$ we have\n",
        "\\begin{align}\n",
        "  (\\lambda a(X) + (1-\\lambda)a'(X) - Y)^{2}\n",
        "  &= \\lambda^{2} (a(X) - Y)^{2} + 2\\lambda(1-\\lambda)(a(X) - Y)(a'(X) - Y)\n",
        "  + (1-\\lambda)^{2}(a'(X) - Y)^2\n",
        "  \\\\\n",
        "  &= \\lambda (a(X) - Y)^{2} - \\lambda(1-\\lambda)((a(X) - Y)^{2} - 2(a(X) - Y)(a'(X) - Y) + (a'(X) - Y)^{2})\n",
        "  + (1-\\lambda)(a'(X) - Y)^2\n",
        "  \\\\\n",
        "  &= \\lambda (a(X) - Y)^{2} - \\lambda(1-\\lambda)((a(X) - Y) - (a'(x) - Y))^{2}\n",
        "  + (1-\\lambda)(a'(X) - Y)^2\n",
        "  \\\\\n",
        "  &= \\lambda (a(X) - Y)^{2} - \\lambda(1-\\lambda)(a(X) - a'(x))^{2}\n",
        "  + (1-\\lambda)(a'(X) - Y)^2.\n",
        "\\end{align}\n",
        "In particular, we have\n",
        "\\begin{align}\n",
        "  r(\\lambda a + (1-\\lambda)a') = \\lambda r(a) - \\lambda(1-\\lambda)\\|a - a'\\|_{L_{2}}^{2} + (1-\\lambda)r(a').\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgodNaonI8Gi"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upQv6AepVNpo"
      },
      "source": [
        "1. Exercise 2.1. The method `compute_empirical_risk` can be implemented via a simple numpy arithmetics:\n",
        "```\n",
        "    return np.average((a(self.X) - self.y)**2)\n",
        "```\n",
        "\n",
        "2. Exercise 2.2. Computing the population risk of a convex combination of dictionary elements $a_{w}$ can be done via a repeated application of the above hint. In particular, assuming that $w_{1} > 0$, we may write\n",
        "$$\n",
        "  a_{w} = \\sum_{i=1}^{n}w_{i}a_{i} = w_{1}a_{1} + (1-w_{1})\\underbrace{\\left(\\sum_{i=2}^{n} \\frac{w_{i}}{1-w_{1}}a_{i}\\right)}_{a'}.\n",
        "$$\n",
        "Thus\n",
        "$$\n",
        "  r(a_{w}) = w_{1}r(w_{1}) - w_{1}(1-w_{1})\\|a_{1} - a'\\|_{L_{2}}^{2}+(1-w_{1})r(a').\n",
        "$$\n",
        "The below code snippet provides a sample implementation:\n",
        "```\n",
        "    # Identify the first non-zero coordinate.\n",
        "    first_idx = np.argmax(w > 0)\n",
        "    mu = w[first_idx]\n",
        "\n",
        "    if isclose(mu, 1.0, abs_tol=1e-8):\n",
        "      return self.r[first_idx]\n",
        "\n",
        "    # Let us express the function a_w as mu * a_{first_idx} + (1-mu) * a_{w'}.\n",
        "    w_prime = np.copy(w)\n",
        "    w_prime[first_idx] = 0\n",
        "    w_prime /= (1.0 - mu)\n",
        "\n",
        "    # Define the basis vector e_{first_idx}.\n",
        "    e_first_idx = np.zeros_like(w)\n",
        "    e_first_idx[first_idx] = 1.0\n",
        "\n",
        "    return mu * self.r[first_idx] + (1.0-mu) * self.compute_r(w_prime) \\\n",
        "        - mu * (1-mu) * self.compute_L2_squared_norm(e_first_idx - w_prime)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY0dYOoSZx9_"
      },
      "source": [
        "### Exercise 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2EsjAokZ0mx"
      },
      "source": [
        "Estimating how the estimation error $\\mathcal{E}(W)$ behaves with increasing sample size $n$ will require multiple calling the class `Problem` method `sample_estimation_errors` once for every sample size $n$. In this exercise, you are asked to implement this functionality by filling in the missing implementation details below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmNYKSIzIOdi"
      },
      "source": [
        "\n",
        "class ConvergenceRateExperiment(object):\n",
        "  \"\"\" A class encapsulating the functionality for estimating estimation error\n",
        "  convergence rates. \"\"\"\n",
        "  \n",
        "  def __init__(self, problems):\n",
        "    \"\"\" :problems: A list of instances of type Problem. \"\"\"\n",
        "    self.problems = problems\n",
        "    self.ns = np.array([problem.n for problem in problems])\n",
        "\n",
        "  def sample_estimation_errors(self, n_seeds, estimator):\n",
        "    \"\"\" :n_seeds: The number of i.i.d. samples of \\mathcal{E}_{problem}(W) that\n",
        "          should be taken for each problem in self.problems.\n",
        "        :estimator: An object of type Estimator (see the abstract class above)\n",
        "          that represents the estimator W.\n",
        "        :returns: A numpy array of shape (len(self.problems), n_seeds) whose\n",
        "          i-th row contains n_seeds i.i.d. samples of the estimation error\n",
        "          for the i-th problem instance.\n",
        "    \"\"\"\n",
        "    ############################################################################\n",
        "    # Exercise 3 - fill in the code below\n",
        "    \n",
        "    ############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYxbRpsxcZWp"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYbWmHuTcblr"
      },
      "source": [
        "The method `sample_estimation_errors` may be implemented as follows:\n",
        "```\n",
        "    estimation_errors = np.zeros((len(self.problems), n_seeds))\n",
        "    for problem_idx, problem in tqdm(\n",
        "        enumerate(self.problems), total=len(self.problems)):\n",
        "      estimation_errors[problem_idx,:] = \\\n",
        "        problem.sample_estimation_errors(estimator, n_seeds)\n",
        "    return estimation_errors\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt6YbGHeHWEH"
      },
      "source": [
        "## Tuning Regularization Parameter of Ridge Regression Estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSqkKVt5MBA1"
      },
      "source": [
        "<font color='green'>**This section is meant to demonstrate one of the many possible applications of model selection aggregation - tuning a regularization parameter on held-out data.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixs9cQq6c0nc"
      },
      "source": [
        "\n",
        "\n",
        "A ridge regression estimator with regularization parameter $\\alpha$ is any solution to the following optimization problem\n",
        "\\begin{equation}\n",
        "  \\min_{w\\in\\mathbb{R}^{d}} \\|Xw - y\\|_{2}^{2} + \\alpha\\|w\\|^{2}_{2},\n",
        "\\end{equation}\n",
        "where the $i$-th row of the matrix $X \\in \\mathbb{R}^{n\\times d}$ equals  the $i$-th covariate vector $X_{i}$ and the $i$-th element of $y$ denotes the $i$-th observed target $Y_{i}$.\n",
        "\n",
        "The next exercise asks you to implement our first subclass of the abstract class `Problem`. Before continuing, please familiarize yourself with the scikit-learn implementation of the ridge regression estimator available at the following hyperlink https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay9KGbJtI0Xe"
      },
      "source": [
        "class ParameterTuningProblem(Problem):\n",
        "  \"\"\" An instance of a model selection problem, where the dictionary of\n",
        "  functions A corresponds to a set of ridge regression estimators with different\n",
        "  regularization parameters.\n",
        "  \"\"\"\n",
        "\n",
        "  # The below class members define the configuration of the dictionary A.\n",
        "  init_seed = 0 # numpy seed used to generate the dictionary A.\n",
        "  d = 100 # Dimension of the covariate vectors.\n",
        "  n_train = 200 # Number of training data points used to generate the dictionary\n",
        "                # A.\n",
        "  noise_std = 2.5 # Standard deviation of the noise. See class method\n",
        "                  # __generate_gaussian_data.\n",
        "  \n",
        "  # The below variable define a list of regularization parameters. Each\n",
        "  # regularization parameter alpha corresponds to one function in the dictionary\n",
        "  # A.\n",
        "  alphas = np.linspace(start=1.0, stop=40.0, num=100)\n",
        "\n",
        "  # The below class members will be initialized during the first instantiation\n",
        "  # of an object of this class. See the __initialize_class_members class method.\n",
        "  w_star = None # The parameter generating the data.\n",
        "  A = None # The list of fitted ridge estimators, one for each regularization\n",
        "           # parameters.\n",
        "  r = None # Population risks of elements of A.\n",
        "  w_alphas = None # A matrix of size len(alphas) \\times d, whose i-th row\n",
        "                  # equals the parameter vector corresponding to the i-th\n",
        "                  # dictionary element a_i.\n",
        "\n",
        "  @classmethod\n",
        "  def __generate_gaussian_data(cls, n, d, w_star):\n",
        "    \"\"\" A method for generating data from the Gaussian model\n",
        "\n",
        "          X_{i} ~ N(0, I_{d})\n",
        "          Y_{i} | X_{i} = x ~ N(<w_star, x>, sigma^{2}),\n",
        "\n",
        "        where the noise standrad deviation sigma is stored as the class member\n",
        "        cls.noise_std.\n",
        "\n",
        "        :n: Number of data points (X_{i}, Y_{i}) to generate.\n",
        "        :d: Dimensionality of the covariate vectors X_{i}.\n",
        "        :w_star: The optimal parameter; i.e., the parameter that generates the\n",
        "          noisy observations.\n",
        "        :returns: The n data points (X_{i}, Y_{i}) stored into n \\times d and\n",
        "          n \\times 1 numpy arrays X and y, respectively.\n",
        "    \"\"\"\n",
        "    ############################################################################\n",
        "    # Exercise 4.1\n",
        "\n",
        "    ############################################################################\n",
        "\n",
        "  @classmethod\n",
        "  def __initialize_class_members(cls):\n",
        "    if cls.w_star is not None and cls.A is not None and cls.r is not None:\n",
        "      # Already initialized.\n",
        "      return\n",
        "\n",
        "    np.random.seed(cls.init_seed)\n",
        "    ############################################################################\n",
        "    # Exercise 4.2. Initialize the class members:\n",
        "    #   - cls.w_star ~ N(0, I_{d})\n",
        "    #   - cls.A - a dictionary of fitted ridge regression estimators.\n",
        "    #   - cls.r - population risks of elements of the dictionary cls.A.\n",
        "    #   - cls.w_alphas - parameter vector values of the fitted ridge estimators.\n",
        "\n",
        "    ############################################################################\n",
        "\n",
        "  def __init__(self, n):\n",
        "    type(self).__initialize_class_members()\n",
        "    super().__init__(type(self).A, type(self).r, n)\n",
        "\n",
        "  def _generate_data(self):\n",
        "    return type(self).__generate_gaussian_data(\n",
        "      self.n, type(self).d, type(self).w_star)\n",
        "    \n",
        "  def compute_L2_squared_norm(self, w):\n",
        "    ############################################################################\n",
        "    # Exercise 4.3\n",
        "    \n",
        "    ############################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLhL2YJSm4oe"
      },
      "source": [
        "### Exercise 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLoyF_aNm6Nn"
      },
      "source": [
        "Fill in the missing implementation details in class `ParameterTuningProblem`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKVPzkGZm_LX"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEgOL1i6nBCl"
      },
      "source": [
        "1. Exercise 4.1. We can generate the Gaussian data via numpy's `np.random.normal` function:\n",
        "```\n",
        "    X = np.random.normal(loc=0.0, scale=1.0, size=(n, d))\n",
        "    y = X @ (w_star.reshape(-1,1)) \\\n",
        "      + np.random.normal(loc=0.0, scale=cls.noise_std, size=(n,1))\n",
        "    return X, y\n",
        "```\n",
        "\n",
        "2. Exercise 4.2. Recall that the data is follows the distribution\n",
        "\\begin{align}\n",
        "  X &\\sim N(0, I_{d}), \\\\\n",
        "  Y \\vert X = x &\\sim N(\\langle w^{*}, x \\rangle, \\sigma^{2}).\n",
        "\\end{align}\n",
        "Hence, the population risk of a linear function $\\langle w, \\cdot \\rangle$ can be computed as follows\n",
        "\\begin{align}\n",
        "  r(\\langle w, \\cdot \\rangle)\n",
        "  &= \\mathbf{E}[(\\langle w, X \\rangle - Y)^{2}]\n",
        "  \\\\\n",
        "  &= \\mathbf{E}[(\\langle w - w^{*}, X \\rangle - Y + \\langle w^{*}, X \\rangle)^{2}]\n",
        "  \\\\\n",
        "  &=  \\mathbf{E}[(\\langle w - w^{*}, X\\rangle^{2}]\n",
        "      +2\\mathbf{E}[\\langle w - w^{*}, X\\rangle (\\langle w^{*}, X\\rangle - Y)] +\n",
        "      \\mathbf{E}[(Y - \\langle w^{*}, X \\rangle)^{2}]\n",
        "  \\\\\n",
        "  &=  \\mathbf{E}[(\\langle w - w^{*}, X\\rangle^{2}]\n",
        "      +2\\mathbf{E}[\\langle w - w^{*}, X\\rangle]\\mathbf{E}[\\langle w^{*}, X\\rangle - Y] +\n",
        "      \\mathbf{E}[(Y - \\langle w^{*}, X \\rangle)^{2}]\n",
        "  \\\\\n",
        "  &=  \\mathbf{E}[(\\langle w - w^{*}, X\\rangle^{2}]\n",
        "      +\n",
        "      \\mathbf{E}[(Y - \\langle w^{*}, X \\rangle)^{2}]\n",
        "  \\\\\n",
        "  &=  \\|w - w^{*}\\|_{2}^{2}\n",
        "      +\n",
        "      \\mathbf{E}[(Y - \\langle w^{*}, X \\rangle)^{2}]\n",
        "  \\\\\n",
        "  &=  \\|w - w^{*}\\|_{2}^{2}\n",
        "      +\n",
        "      \\sigma^{2}.\n",
        "\\end{align}\n",
        "The below code snipped provides one possible solution to Exercise 4.2:\n",
        "```\n",
        "    # Set the parameter w_star that will be used to generate the datasets.\n",
        "    cls.w_star = np.random.normal(loc=0.0, scale=1.0, size=(cls.d,))\n",
        "    # Generate training data.\n",
        "    X, y = cls.__generate_gaussian_data(cls.n_train, cls.d, cls.w_star)\n",
        "\n",
        "    # The following function will be used to construct dictionary elements from\n",
        "    # fitted ridge estimators.\n",
        "    def get_linear_function(w):\n",
        "      def apply_w(X):\n",
        "        return X.reshape(X.shape[0], -1) @ w.reshape(-1, 1)\n",
        "      return apply_w\n",
        "\n",
        "    # For each regularization parameter in cls.alphas, fit ridge regression\n",
        "    # estimator to the data (X, y) and populate the arrays cls.A, cls.r and\n",
        "    # cls.w_alphas.\n",
        "    cls.w_alphas = np.zeros((len(cls.alphas), cls.d))\n",
        "    cls.A = []\n",
        "    cls.r = []\n",
        "    for alpha_idx, alpha in enumerate(cls.alphas):\n",
        "      ridge_estimator = Ridge(alpha=alpha, fit_intercept=False, copy_X=True)\n",
        "      ridge_estimator.fit(X,y.flatten())\n",
        "      fitted_w = ridge_estimator.coef_\n",
        "      cls.w_alphas[alpha_idx,:] = fitted_w\n",
        "      cls.A.append(get_linear_function(fitted_w))\n",
        "      cls.r.append(np.sum((fitted_w - cls.w_star)**2) + cls.noise_std**2)\n",
        "```\n",
        "\n",
        "3. Exercise 4.3.\n",
        "Denote the $i$-th element of the dictionary $\\mathcal{A}$ by $a_{i} = \\langle w_{\\alpha_{i}}, \\cdot \\rangle$, where $w_{\\alpha_{i}}$ denotes the output of the ridge estimator with regularization parameter $\\alpha_{i}$. Then, for $w \\in \\mathbb{R}^{m}$, we have\n",
        "\\begin{align*}\n",
        "  \\|a_{w}\\|_{L_{2}}^{2}\n",
        "  = \\mathbf{E}[a_{w}(X)^{2}]\n",
        "  = \\mathbf{E}\\left[\\left\\langle \\sum_{i=1}^{m} w_{i}w_{\\alpha_{i}}, X \\right\\rangle^{2}\\right]\n",
        "  = \\left\\| \\sum_{i=1}^{m} w_{i}w_{\\alpha_{i}} \\right\\|_{2}^{2}.\n",
        "\\end{align*}\n",
        "Hence, Exercise 4.3 may be implemented, for example, via the following two lines of code:\n",
        "```\n",
        "     aggregated_w = np.transpose(type(self).w_alphas) @ w.reshape(-1,1)\n",
        "     return np.sum(aggregated_w**2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHvfp-Oft495"
      },
      "source": [
        "### Sanity Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE5FO19YuA4L"
      },
      "source": [
        "We have already implemented quite a lot of code without verifying its correcness. The next exercise asks us to perform a few simple tests to see if the classes `Problem` and `ParameterTuningProblem` behave as expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv_8DD1Wuazd"
      },
      "source": [
        "#### Exercise 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIgK5foZugG_"
      },
      "source": [
        "- Instantiate an object of type `ParameterTuningProblem` with `n=100`.\n",
        "- Plot the population risks of the dictionary `ParameterTuningProblem.A`.\n",
        "- Plot the empirical risks of the dictionary `ParameterTuningProblem.A`.\n",
        "- Check if your implementation of the method `Problem.compute_r` agrees with the\n",
        "exact expression of the population risk given in solutions to Exercise 4.2 and Exercise 4.3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij_e7mAwwIAg"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5zLT4vjwKOb"
      },
      "source": [
        "The bellow code cell fulfills the first three bullet points of Exercise 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifSsabIZVpSC"
      },
      "source": [
        "problem = ParameterTuningProblem(n=100)\n",
        "\n",
        "# Plot the population risks.\n",
        "plt.plot(ParameterTuningProblem.r)\n",
        "plt.scatter(np.argmin(ParameterTuningProblem.r), np.min(ParameterTuningProblem.r))\n",
        "\n",
        "# Compute empirical risks.\n",
        "R = np.zeros(len(problem.A))\n",
        "for i in range(len(R)):\n",
        "  R[i] = problem.compute_empirical_risk(problem.A[i])\n",
        "\n",
        "# Plot the empirical risks.\n",
        "plt.plot(R)\n",
        "plt.scatter(np.argmin(R), np.min(R))\n",
        "plt.legend([r'population risk $r(a_{i})$', r'empirical risk $R(a_{i})$'])\n",
        "plt.xlabel(r'Model index $i$')\n",
        "plt.ylabel(r'Risk')\n",
        "plt.title('Population vs empirical risks')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbBSoiLowRB4"
      },
      "source": [
        "The below code cell fullfills the final bullet point of Exercise 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb_MXhFue469"
      },
      "source": [
        "# Let us repeat the test three times with a different seed.\n",
        "for seed in range(3):\n",
        "  np.random.seed(seed)\n",
        "  print('seed', seed)\n",
        "\n",
        "  # Generate a random vector mu that will be used to define a convex combination\n",
        "  # of dictionary elements.\n",
        "  w = np.random.normal(0,1,len(type(problem).A))\n",
        "  w = np.abs(w)\n",
        "  w = w / np.sum(w)\n",
        "\n",
        "  # Compute r(a_w) using our implementation from Exercise 2.2.\n",
        "  generic_method_output = problem.compute_r(w)\n",
        "\n",
        "  # Now compute r(a_w) using exact formulas available in Exercise 4.2 and\n",
        "  # Exercise 4.3.\n",
        "  mixture_w = np.transpose(problem.w_alphas) @ w.reshape(-1,1)\n",
        "  exact_computation_output = \\\n",
        "    np.sum((mixture_w - problem.w_star.reshape(-1,1))**2) \\\n",
        "    + problem.noise_std**2\n",
        "\n",
        "  # The below output should be accurate at least to 7 decimal places.\n",
        "  error = generic_method_output - exact_computation_output\n",
        "  print('accuracy', error)\n",
        "  # Make sure that the below assertion does not fail.\n",
        "  assert isclose(error, 0.0, abs_tol=1e-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aylwdZ7NNVpi"
      },
      "source": [
        "## Empirical Risk Minimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yftrfHtz3omR"
      },
      "source": [
        "Now that we have one problem instance implementation ready (i.e., the class `ParameterTuningProblem`), we are ready to implement our first aggregation algorithm.\n",
        "\n",
        "Given a function class $\\mathcal{A}$, an *empirical risk minimizer* $A$ is any solution to the equation\n",
        "$$\n",
        "  A \\in \\mathrm{argmin}_{a \\in \\mathcal{A}} R(A) =  \\mathrm{argmin}_{a \\in \\mathcal{A}}\\frac{1}{n} \\sum_{i=1}^{n}(a(X_{i}) - Y_{i})^{2}.\n",
        "$$\n",
        "In the context of model selection aggregation, empirical risk minimization corresponds to selecting the estimator with lowest error on the held-out validation dataset. The next exercise asks us to implement the empirical risk minimization estimator and estimate its in-expectation estimation error behaviour as a function of the sample size $n$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN7wX5BQNYBk"
      },
      "source": [
        "class ERM(Estimator):\n",
        "  \"\"\" An implementation of the empirical risk minimization estimator. \"\"\"\n",
        "\n",
        "  def fit(self, problem):\n",
        "    \"\"\" See the base class documentation. \"\"\"\n",
        "    ############################################################################\n",
        "    # Exercise 6.1 - fill in the code below.\n",
        "\n",
        "    ############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mz0eVct-BVB"
      },
      "source": [
        "### Exercise 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdUX1U95-FMy"
      },
      "source": [
        "- Exercise 6.1.\n",
        "  Fill in the missing implementation details of the class ERM defined in the above cell.\n",
        "- Exercise 6.2. Let $(\\mathcal{A}, P)$ be implemented via the class `ParameterTuningProblem` and let $W$ be implemented via the class `ERM`. Evaluate empirically the rate of decay of the expected estimation error $\\mathbf{E}_{Z_{1}^{n}}[\\mathcal{E}_{n, P, \\mathcal{A}}(W)]$ as a function of sample size $n$ in the range `ns = np.arange(100, 200, step=10)`. Hint: make use of the `ConvergenceRateExperiment` class.\n",
        "- Exercise 6.3. In the cell defining `ParameterTuningProblem`, try changing the number of regularization parameters $m$, i.e., modify the parameter `num` in line `alphas = np.linspace(start=1.0, stop=40.0, num=100)` (you will need to rerun the cell defining the class `ParameterTuningProblem`). Is the expected estimation error $\\mathbf{E}_{Z_{1}^{n}}[\\mathcal{E}_{n, P, \\mathcal{A}}(W)]$ sensitive to the size of the dictionary $m = |\\mathcal{A}|$?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8chwg4PCy9c"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7xI3J6WC1U7"
      },
      "source": [
        "Exercise 6.1. The ERM estimator can be implemented as follows:\n",
        "```\n",
        "      R = np.zeros(len(problem.A))\n",
        "      for idx, a in enumerate(problem.A):\n",
        "          R[idx] = problem.compute_empirical_risk(problem.A[idx])\n",
        "      w = np.zeros(len(problem.A))\n",
        "      w[np.argmin(R)] = 1.0\n",
        "      return w\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgDSx1ZAEKfW"
      },
      "source": [
        "The below code cell implements a possible solution to Exercise 6.2. Inspecting the generated plot, we can visually conjecture that for a fixed pair $(P, \\mathcal{A})$ implemented via the class `ParameterTuningProblem`, the expected estimation error decays as fast as $\\mathbf{E}_{Z_{1}^{n}}[\\mathcal{E}_{n, P, \\mathcal{A}}(W)] = O(1/n)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzS8FOl_ZRdg"
      },
      "source": [
        "    ridge_problems = [ParameterTuningProblem(n) for n in \\\n",
        "                      np.arange(100, 200, step=10)]\n",
        "    ridge_tuning_experiment = ConvergenceRateExperiment(ridge_problems)\n",
        "    estimation_errors = ridge_tuning_experiment.sample_estimation_errors(\n",
        "      n_seeds=1000, estimator=ERM())\n",
        "\n",
        "    # Generate a log-log plot of sample size vs expected estimation error.\n",
        "    plt.plot(np.log(ridge_tuning_experiment.ns),\n",
        "             np.log(np.mean(estimation_errors, axis=1)))\n",
        "    plt.xlabel(r'$\\log n$')\n",
        "    plt.ylabel(r'$\\log \\mathbf{E}_{Z_{1}^{n}}[\\mathcal{E}(W)]$')\n",
        "    plt.title('Expected Estimation Error Convergence Rate')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4-YTpVhFyOd"
      },
      "source": [
        "Exercise 6.3. Rerunning the above cell a few times after modifying the number of regularization parameters $m$ in the class `ParameterTuningProblem` we conclude that the expected estimation error is not sensitive to the size of the dictionary. <font color='green'>**It is thus plausible, that the empirical risk minimization estimator achieves the expectation-optimal rate $(2)$ (refer to the introduction for details) and at this point we cannot conclude that it is suboptimal. However, recall the minimax notion of optimality requires that the $O(\\log m/n)$ expected estimation-error rate holds for any problem (with the constraint that $\\mathcal{Y} = [-1, 1])$) and thus, at this point, we cannot conclude that ERM is minimax optimal either.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYTexf3oOT48"
      },
      "source": [
        "## A Hard Problem Instance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TJa2gaCHpRJ"
      },
      "source": [
        "In the previous section, we have investigated a model selection aggregation problem $(P, \\mathcal{A})$, for which (it appears that) the empirical risk minimization algorithm attains the optimal rate $O(\\log m/n)$. Recall, however, that the notion of optimality introduced in the introduction requires that the rate $O(\\log m/n)$ holds *uniformly* over all problems $(P, \\mathcal{A})$ and thus we cannot yet conclude that the empirical risk minimization estimator is expectation-optimal.\n",
        "\n",
        "In this section we will show that aggregation via empirical risk minimization (ERM) does, in fact, result in a sub-optimal aggregation procedure. <font color='green'>**To achieve this goal, we first discuss a simple problem that is provably difficult for any algorithm -- identifying whether a coin is biased or not given an insufficient number of coin flips.**</font> We will then leverage the fact that the ERM estimator is constrained to output a function in the set $\\{a_{1}, \\dots, a_{m}\\}$ to informally deduce that an in-expectation convergence rate of order $o(1/\\sqrt{n})$ would imply the existence of an estimator that solves an impossible problem. By contradiction, the in-expectation performance of ERM on the ``biased coin  problem'' has to decay as slow as $\\Omega(1/\\sqrt{n})$, thus proving that the ERM estimator is sub-optimal.\n",
        "\n",
        "<font color='green'>**We remark that the limitation of ERM discussed in this section applies to all *proper* algorithms**</font>, also called *selectors* in the context of empirical risk minimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ53Dnu4PLtf"
      },
      "source": [
        "### Exercise 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-KaUZa1PPOv"
      },
      "source": [
        "- Exercise 7.1. Let $P_{1}$ and $P_{2}$ denote $\\mathrm{Bernoulli}(p_{1})$ and $\\mathrm{Bernoulli}(p_{2})$ distributions, with $p_{1} \\in \\{0.5, 0.5 + \\gamma\\}$, $p_{2} \\in \\{0.5, 0.5 + \\gamma\\}$, $p_{1} \\neq p_{2}$ and $\\gamma \\in (0, 0.5)$. Thus, one of the distributions represents an unbiased coin while the other distribution represents a $\\gamma$-biased coin. Suppose that we observe a sequence of $n$ i.i.d. coin flips $X_{1}, \\dots, X_{n} \\sim P_{1}$ and $n$ i.i.d. coin flips $Y_{1}, \\dots, Y_{n} \\sim P_{2}$.\n",
        "\n",
        "  For what values of $\\gamma$ can you identify which of the two coins is biased with high confidence (for example, by using the principle of empirical risk minimization)? You are only asked to provide an informal argument.\n",
        "\n",
        "- Exercise 7.2. Using your answer to Exercise 7.1, suggest a dictionary of two functions $\\mathcal{A} = \\{a_{1}, a_{2}\\}$ and a distribution $P$ such that the ERM estimator $W$ incurs the sub-optimal rate $\\mathbf{E}_{Z_{1}^{n}}[\\mathcal{E}_{n,P,\\mathcal{A}}(W)] = \\Omega(1/\\sqrt{n}) \\gg \\Theta(\\log (m)/n)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIVyR3NMSjnw"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcVzzVgESqGR"
      },
      "source": [
        "- Exercise 7.1. Assume without loss of generality that the first coin is biased (i.e., $p_{1} = 0.5 + \\gamma$) and the second coin is unbiased (i.e., $p_{2} = 0.5$). Let\n",
        "$$\\widehat{P}_{1} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i} \\quad\\text{and}\\quad \\widehat{P}_{2} = \\frac{1}{n} \\sum_{i=1}^{n} Y_{i}.$$\n",
        "We already know that by [Chernoff bounds](https://en.wikipedia.org/wiki/Chernoff_bound), the following two inequalities hold simultaneously with probability at least $1 - \\delta$\n",
        "\\begin{equation}\n",
        "  \\left| (0.5 + \\gamma) - \\widehat{P}_{1} \\right| \\leq O\\left(\\sqrt{\\frac{\\log(1/\\delta)}{n}}\\right)\n",
        "  \\quad\\text{and}\\quad\n",
        "  \\left| 0.5 - \\widehat{P}_{2} \\right| \\leq O\\left(\\sqrt{\\frac{\\log(1/\\delta)}{n}}\\right).\n",
        "\\end{equation}\n",
        "In particular, if $\\gamma = \\Omega\\left(\\sqrt{\\frac{\\log(1/\\delta)}{n}}\\right)$, then the empirical risk minimization strategy that returns $\\mathrm{argmax}_{i=1,2}\\{\\widehat{P}_{1}, \\widehat{P}_{2}\\}$ succeeds with probability at least $1-\\delta$.\n",
        "\n",
        "  In the other direction, it is possible to show that $n = \\Omega(1/\\gamma^{2})$ coin flips are necessary to identify the biased coin for *any* algorithm. For further details see Lecture 16 and Problem 4.4 of the [Algorithmic Foundations of Learning course](http://www.stats.ox.ac.uk/~rebeschi/teaching/AFoL/20/).\n",
        "\n",
        "- Exercise 7.2. We need to ``embed'' the biased coin problem described in Exercise 7.1 into the model selection aggregation framework. To do this, consider a distribution $P$ such that $(a_{1}(X) - Y)^2$ is proportional to $\\mathrm{Bernoulli}(0.5 - \\gamma)$ random variable and $(a_{2}(X) - Y)^{2}$ is proportional to $\\mathrm{Bernoulli}(0.5)$ random variable. For instance, this is achieved by letting\n",
        "\\begin{align}\n",
        "  X = (X_1, X_2) \\in \\mathbb{R}^{2} &\\sim \\text{i.i.d. symmetric }\\{-1, +1\\}\\text{ random variables}, \\\\\n",
        "  Y \\vert X = (x_1, x_2) &\\sim \\begin{cases}\n",
        "    x_{1} & \\text{with probability 0.5 + } \\gamma, \\\\\n",
        "    -x_{1} & \\text{with probability 0.5 - }\\gamma.\n",
        "  \\end{cases}\n",
        "\\end{align}\n",
        "and\n",
        "$$\n",
        "  a_{1}(x) = x_{1} \\quad\\text{and}\\quad a_{2}(x) = x_{2}.\n",
        "$$\n",
        "Thus, the function $a_{1}$ aggrees with the label $Y$ exactly with probability $0.5 + \\gamma$, while the function $a_{2}$ agrees with the label $Y$ half the time. It follows that\n",
        "$$\n",
        "  r(a_1) = 2 - 4\\gamma \\quad\\text{and}\\quad r(a_2) = 2.\n",
        "$$\n",
        "The expected estimation error of the empirical risk minimization algorithm $W$ can be now written as\n",
        "\\begin{equation}\n",
        "  \\mathbf{E}_{Z_{1}^{n}}[\\mathcal{E}(W)] = 4\\gamma \\mathbb{P}_{Z_{1}^{n}}(W = e_{2}).\n",
        "\\end{equation}\n",
        "Let $\\gamma = c_{1}/\\sqrt{n}$ for some absolute constant $c_{1}$ and suppose that  $\\mathbf{E}_{Z_{1}^{n}}[\\mathcal{E}(W)] = O(n^{-(1/2 + \\alpha)})$ for some constant $\\alpha > 0$. It follows that $\\mathbb{P}_{Z_{1}^{n}}(W = e_{2}) = O(n^{-\\alpha})$. But this is impossible given our (informal) discussion in the solution of Exercise 7.1 that suggests the contradictory equation\n",
        "$$c_{1}/\\sqrt{n} = \\gamma = \\Omega(\\sqrt{\\alpha \\log (n) / n}).$$\n",
        "  We conclude that it must be the case that $\\mathbf{E}_{Z_{1}^{n}}[\\mathcal{E}(W)] = \\Omega(1/\\sqrt{n})$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmah_-uHjqwp"
      },
      "source": [
        "### Implementing the Biased Coin Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnB0PlLmkaLf"
      },
      "source": [
        "We now turn to the implementation of the ``hard'' model selection aggregation problem instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDQVN4VvNQq8"
      },
      "source": [
        "class BiasedCoinProblem(Problem):\n",
        "  \"\"\" An implementation of the biased coin problem described in the solution\n",
        "  of Exercise 7.2.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, n, m, gamma):\n",
        "    \"\"\" :n: Sample size.\n",
        "        :m: The number of coins, m-1 of which are unbiased.\n",
        "        :gamma: The bias of the biased coin.\n",
        "    \"\"\"\n",
        "    self.m = m\n",
        "    ############################################################################\n",
        "    # Exercise 8.1\n",
        "    # Write code defining the following two variables:\n",
        "    #   - A: a list containg m functions representing the coins;\n",
        "    #   - r: a list containing the population risks of the functions in A.\n",
        "\n",
        "    ############################################################################\n",
        "    super().__init__(A, r, n)\n",
        "\n",
        "\n",
        "  def _generate_data(self):\n",
        "    ############################################################################\n",
        "    # Exercise 8.2\n",
        "\n",
        "    ############################################################################\n",
        "\n",
        "  def compute_L2_squared_norm(self, w):\n",
        "    ############################################################################\n",
        "    # Exercise 8.3\n",
        "    \n",
        "    ############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WX55yvRkQ5q"
      },
      "source": [
        "#### Exercise 8\n",
        "\n",
        "- Exercise 8.1, 8.2 and 8.3. Generalize the argument of solution to the Exercise 7.2 to $m$ functions/coins and implement the missing details of the  `BiasedCoinProblem` class in the above cell.\n",
        "- Exercise 8.4. Verify empirically the conclusion of Exercise 7.2. More specifically, for the problem instance $(P, \\mathcal{A})$ implemented by the class `BiasedCoinProblem` (with $m=2$) and the empirical risk minimization estimator $W$, show empirically that $\\mathbf{E}_{Z_{1}^{n}}[\\mathcal{E}_{n,P,\\mathcal{A}}(W)] = \\Omega(1/\\sqrt{n})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMMUElmLoC0c"
      },
      "source": [
        "##### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL1ljIQG6z0Y"
      },
      "source": [
        "- Exercise 8.1.\n",
        "\n",
        "```\n",
        "    # Given a matrix X, the i-th coin returns the i-th column of X.\n",
        "    def get_a(idx):\n",
        "      def a_idx(X):\n",
        "        return X[:,idx].reshape(-1, 1)\n",
        "      return a_idx\n",
        "\n",
        "    # Construct the coins.\n",
        "    A = [get_a(idx) for idx in range(m)]\n",
        "\n",
        "    # Set the bias of the biased coin and fill in the population risks list a.\n",
        "    self.gamma=gamma\n",
        "    r = np.ones(m) * 0.5\n",
        "    r[0] -= self.gamma\n",
        "```\n",
        "\n",
        "- Exercise 8.2.\n",
        "\n",
        "```\n",
        "    X = np.random.binomial(n=1, p=0.5, size=(self.n, self.m))\n",
        "    y = X[:,0].copy().reshape(-1, 1)\n",
        "\n",
        "    biased_mask = np.random.binomial(n=1, p=0.5 - self.gamma, size=(self.n,1))\n",
        "    # If biased_mask[i] = 1, y[i] will be flipped. Otherwise y[i] remains\n",
        "    # unchanged.\n",
        "    y += biased_mask\n",
        "    y %= 2\n",
        "    \n",
        "    return X, y    L2_squared_norm = 1.0/4.0 * np.sum(w)**2\n",
        "    L2_squared_norm += 1.0/4.0 * np.sum(w**2)\n",
        "```\n",
        "\n",
        "- Exercise 8.3. Let $I_{m} \\in \\mathbb{R}^{m \\times m}$ denote the $m \\times m$ identity matrix. Notice that\n",
        "\\begin{align*}\n",
        "  \\|a_{w}(X)\\|_{L_2(P)}^{2}\n",
        "  &= \\mathbf{E}\\left[\\langle w, X \\rangle^{2}\\right]\n",
        "  \\\\\n",
        "  &= w^{\\mathsf{T}}\\mathbf{E}\\left[XX^{\\mathsf{T}}\\right]w\n",
        "  \\\\\n",
        "  &= w^{\\mathsf{T}}I_{m}w\n",
        "  \\\\\n",
        "  &= \\|w\\|_{2}^{2}.\n",
        "\\end{align*}\n",
        "Thus, the solution to this exercise may be implemented as follows:\n",
        "```\n",
        "    return np.sum(w**2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9RcqmKG6qF_"
      },
      "source": [
        "A possible implementation of Exercise 8.4 is provided in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDzOGVhNORCV"
      },
      "source": [
        "biased_coin_problems = [BiasedCoinProblem(n=n, m=2, gamma=0.5/np.sqrt(n)) \\\n",
        "                        for n in np.arange(100, 200, step=10)]\n",
        "biased_coin_experiment = ConvergenceRateExperiment(biased_coin_problems)\n",
        "erm_errors = biased_coin_experiment.sample_estimation_errors(\n",
        "    n_seeds=10000, estimator=ERM())\n",
        "# Now generate a log-log plot of mean errors vs the sample size.\n",
        "# Note that the slope generated in the plot below is approximately equal to -0.5\n",
        "# suggesting that the mean estimation error scales as \\Omega(1/\\sqrt{n}). \n",
        "plt.plot(np.log(biased_coin_experiment.ns), np.log(np.mean(erm_errors, axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_5RrZLZhIaz"
      },
      "source": [
        "## Exponential Weights Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3GVIvLnhNWU"
      },
      "source": [
        "In the previous section we have established that empirical risk minimization (or any other proper algorithm) does not yield an in-expectation optimal estimator for the problem of model selection aggregation.\n",
        "<font color='green'>**In Exercise 9, we will derive the exponential weights algorithm that achieves the in-expectation goal of optimal model selection aggregation. By our previous discussions, such an esimator is necessarily improper.**</font>\n",
        "\n",
        "Recall that an estimator $W$ is expectation-optimal if \n",
        "\\begin{equation}\n",
        "  \\mathbf{E}\\, r(W) - \\min_{i=1,\\dots,m} r(e_{i}) \\leq c \\frac{\\log m}{n}\n",
        "  \\tag{4}\n",
        "\\end{equation}\n",
        "for some absolute constant $c$. Exercise 9 will guide you through a series of derivations that yield an upper bound of the form\n",
        "$$\n",
        "- \\min_{i=1,\\dots,m} r(e_{i}) \\leq \\dots \\text{ (Exercise 9) }\\dots \\leq \\mathbf{E}_{Z_1^{n}}r(W(Z_{1}, \\dots, Z_{n})) + c \\frac{\\log m}{n}.\n",
        "$$\n",
        "The resulting $W = W(Z_{1},\\dots, Z_{n})$ will then provide us with the desired expectation-optimal estimator.\n",
        "\n",
        "Before we proceed, we introduce some additional notation. For any fixed $w \\in \\mathbb{R}^{m}$, it will be convenient to rewrite\n",
        "$$\n",
        "  r(w) = \\mathbf{E}_{Z}[(a_{w}(X) - Y)^{2}] = \\mathbf{E}_{Z_{n+1}}[(a_{W}(X_{n+1}) - Y_{n+1})^{2}],\n",
        "$$\n",
        " where we have relabelled the sample $Z$ to $Z_{n+1}$. Finally, we let\n",
        "$$\n",
        "  R_{n+1}(w) = \\frac{1}{n+1}\\sum_{i=1}^{n+1}(a_{w}(X_{i}) - Y_{i})^{2}\n",
        "$$\n",
        "and thus remark that for any fixed $w$ (i.e., for $w$ independent of $Z_{1}, \\dots, Z_{n+1}$) the following identity is true\n",
        "$$\n",
        "  r(w) = \\mathbf{E}_{Z_{1}^{n+1}} R_{n+1}(w).\n",
        "$$\n",
        "We are now ready to introduce Exercise 9.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9Wz4WOWkGoR"
      },
      "source": [
        "### Exercise 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn7AoehMklaB"
      },
      "source": [
        "Recall that in this section we work under the assumption $\\mathcal{Y} = [-1, 1]$.\n",
        "\n",
        "- Exercise 9.1. Show that for any $\\lambda > 0$ the following inequality holds:\n",
        "\\begin{equation}\n",
        "\\tag{5}\n",
        "-\\min_{i=1,\\dots,m} \\mathbf{E}_{Z_{1}^{n+1}} R_{n+1}(e_{i}) \\leq   \\mathbf{E}_{Z_{1}^{n+1}} \\left[ \\frac{1}{\\lambda}\\log \\left(\\sum_{i=1,\\dots,m} \\exp(-\\lambda R_{n+1}(e_{i})) \\right) \\right].\n",
        "\\end{equation}\n",
        "- Exercise 9.2. Use the inequality of Exercise 9.1 to find an estimator $W$ that satisfies the expectation-optimal rate stated in equation $(4)$ with the absolute constant $c=8$.\n",
        "\n",
        "  Hint: you may use without proof the fact that for any $\\lambda \\in (0, 1/8)$ and any $y \\in \\mathcal{Y}$, the function $f_{y} : \\mathcal{Y} \\to \\mathbb{R}$ given by $f_{y}(y') = \\exp(-\\lambda(y - y')^{2})$ is concave. In particular, for any $y, y'_{1}, \\dots, y'_{m} \\in \\mathcal{Y}$ and $u_{1}, \\dots, u_{m} \\geq 0$ satisfying $\\sum_{i=1}^{m} u_{i} = 1$, it holds that $\\sum_{i=1}^{n}u_{i}f_{y}(y_{i}) \\leq f_{y}(\\sum_{i=1}^{m}u_{i}y_{i})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9m8AB88kI4Y"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opfovluFqbyw"
      },
      "source": [
        "- Exercise 9.1. We use the technique of approximating maximum by a finite sum inside a logirthm:\n",
        "\\begin{align}\n",
        "  - \\min_{i=1,\\dots,n} \\mathbf{E}_{Z_{1}^{n+1}} [R_{n+1}(e_{i})]\n",
        "  &= \\max_{i=1,\\dots,n} \\mathbf{E}_{Z_{1}^{n+1}} [-R_{n+1}(e_{i})]\n",
        "  \\\\\n",
        "  &\\leq\n",
        "  \\mathbf{E}_{Z_{1}^{n+1}} \\left[ \\max_{i=1,\\dots,m} -R_{n+1}(e_{i}) \\right]\n",
        "  \\\\\n",
        "  &=\n",
        "  \\mathbf{E}_{Z_{1}^{n+1}} \\left[ \\frac{1}{\\lambda}\\log \\left(\n",
        "     \\max_{i=1,\\dots,m} \\exp(-\\lambda R_{n+1}(e_{i})) \\right) \\right]\n",
        "  \\\\\n",
        "  &\\leq\n",
        "  \\mathbf{E}_{Z_{1}^{n+1}} \\left[ \\frac{1}{\\lambda}\\log \\left(\n",
        "     \\sum_{i=1,\\dots,m} \\exp(-\\lambda R_{n+1}(e_{i})) \\right) \\right].\n",
        "\\end{align}\n",
        "\n",
        "- Exercise 9.2. We follow the proof idea outlined at the introduction of this section, namely, we will attempt to further develop the right hand side of equation $(5)$ into\n",
        "$$\n",
        "  -\\min_{i=1,\\dots,m} r(e_{i})  \\leq -\\mathbf{E}_{Z_{1}^{n+1}}(a_{W}(X_{n+1}) - Y_{n+1})^{2} + c\\frac{\\log m}{n}\n",
        "$$\n",
        "for some absolute constant $c>0$ and some estimator $W$ (i.e., a random variable that depends only on $Z_{1},\\dots,Z_{n}$ and not on $Z_{n+1})$). If the latter condition is satisfied, then\n",
        "$$\n",
        "  \\mathbf{E}_{Z_{1}^{n+1}}(a_{W}(X_{n+1}) - Y_{n+1})^{2}\n",
        "  = \\mathbf{E}_{Z_{1}^{n}}[\\mathbf{E}_{Z_{n+1}}[(a_{W}(X_{n+1}) - Y_{n+1})^{2} \\vert Z_{1}, \\dots, Z_{n}]] = \\mathbf{E}_{Z_{1}^{n}}[r(W) \\vert Z_{1}, \\dots, Z_{n}].\n",
        "$$\n",
        "\n",
        "  Before proceeding with the solution let us define some additional notation. For $i=0,\\dots,n+1$ define the empirical risk computed on the first $i$ samples and normalized by $n+1$ by\n",
        "  $$\n",
        "    R_{i}(a) = \\frac{1}{n+1}\\sum_{j=1}^{i}(a(X_{j}) - Y_{j})^{2},\n",
        "  $$\n",
        "  with the convention that $R_{0}(\\cdot) = 0$.\n",
        "  Further, for $i\\in\\{0,\\dots,m\\}$ and $j\\in{1,\\dots,n}$ define the random variables\n",
        "  $$\n",
        "    U^{(j)}_{i} = \\exp(-\\lambda R_{j}(e_{i}))\n",
        "    \\quad\\text{and}\\quad\n",
        "    \\overline{U}^{(j)}_{i} = \\frac{U^{(j)}_{i}}{\\sum_{k=1}^{m}U^{(j)}_{k}}.\n",
        "  $$\n",
        "  \n",
        "  <font color='green'>**The key insight into the solution of this exercise is the following identity\n",
        "  that holds for the right hand side of equation $(5)$**</font>:\n",
        "  $$\n",
        "    \\log\\left(\\sum_{i=1}^{m}U^{(n+1)}_{i}\\right)\n",
        "    = \\log\\left(\\sum_{i=1}^{m}\\exp(-\\lambda R_{n+1}(e_{i}))\\right)\n",
        "    = \\log\\left(\\sum_{i=1}^{m}U^{(n)}_{i}\\exp\\left(-\\frac{\\lambda}{n+1}(Y_{n+1} - a_{i}(X_{n+1}))^{2}\\right)\\right).\n",
        "  $$\n",
        "  In particular, notice that if we could cancel the $\\exp$ and $\\log$ functions,\n",
        "  then we would arrive to an an expression of weighted losses of the dictionary elements $\\{a_{1},\\dots,a_{m}\\}$ with weights that only depend on the first $n$ data points. While the cancelling of $\\exp$ and $\\log$ is not justified for the weights $(U^{(n)}_{i})_{i=1}^{m}$, it is justified by the hint for the weights $(\\overline{U}^{(n)}_{i})_{i=1}^{m}$. This suggests to rewrite the above equation as follows:\n",
        "  \\begin{align}\n",
        "    \\log\\left(\\sum_{i=1}^{m}U^{(n+1)}_{i}\\right)\n",
        "    &=\n",
        "    \\log\\left(\\frac{\\sum_{i=1}^{m}U^{(n+1)}_{i}}{\\sum_{i=1}^{m}U^{(n)}_{i}}\\right)\n",
        "    + \\log\\left(\\sum_{i=1}^{m}U^{(n)}_{i}\\right)\n",
        "    \\\\\n",
        "    &=\n",
        "    \\log\\left(\\sum_{i=1}^{m}\\overline{U}^{(n)}_{i}\\exp\\left(-\\frac{\\lambda}{n+1}(Y_{n+1} - a_{i}(X_{n+1}))^{2}\\right)\\right)\n",
        "     + \\log\\left(\\sum_{i=1}^{m}U^{(n)}_{i}\\right)\n",
        "    \\\\\n",
        "    &\\leq\n",
        "    \\log\\left(\\exp\\left(-\\frac{\\lambda}{n+1}\\left(Y_{n+1} - \\sum_{i=1}^{m}\\overline{U}^{(n)}_{i}a_{i}(X_{n+1})\\right)^{2}\\right)\\right)\n",
        "     + \\log\\left(\\sum_{i=1}^{m}U^{(n)}_{i}\\right)\n",
        "     \\\\\n",
        "    &=\n",
        "    -\\frac{\\lambda}{n+1}\\left(Y_{n+1} - \\sum_{i=1}^{m}\\overline{U}^{(n)}_{i}a_{i}(X_{n+1})\\right)^{2}\n",
        "     + \\log\\left(\\sum_{i=1}^{m}U^{(n)}_{i}\\right)\n",
        "    \\\\\n",
        "    &=\n",
        "    -\\frac{\\lambda}{n+1}\\left(Y_{n+1} - a_{\\overline{U}^{(n)}}(X_{n+1})\\right)^{2}\n",
        "     + \\log\\left(\\sum_{i=1}^{m}U^{(n)}_{i}\\right),\n",
        "  \\end{align}\n",
        "  where the inequality step follows by applying the hint for $\\lambda \\leq (n+1)/8$.\n",
        "  Plugging the above into equation $(5)$ yields\n",
        "  \\begin{align}\n",
        "   -\\min_{i=1,\\dots,m} \\mathbf{E}_{Z_{1}^{n+1}} R_{n+1}(e_{i})\n",
        "   &\\leq   \\mathbf{E}_{Z_{1}^{n+1}} \\left[ \\frac{1}{\\lambda}\\log \\left(\\sum_{i=1,\\dots,m} \\exp(-\\lambda R_{n+1}(e_{i})) \\right) \\right]\n",
        "   \\\\\n",
        "   &=   \\mathbf{E}_{Z_{1}^{n+1}} \\left[ \\frac{1}{\\lambda}\\log \\left(\\sum_{i=1}^{m}U^{(n+1)}_{i} \\right) \\right]\n",
        "   \\\\\n",
        "   &\\leq\n",
        "   -\\frac{1}{n+1}\\mathbf{E}_{Z_{1}^{n+1}}\\left[\n",
        "\\left(Y_{n+1} - a_{\\overline{U}^{(n)}}(X_{n+1})\\right)^{2}\n",
        "     \\right]\n",
        "     + \\mathbf{E}_{Z_{1}^{n+1}} \\left[ \\frac{1}{\\lambda}\\log \\left(\\sum_{i=1}^{m}U^{(n)}_{i} \\right) \\right]\n",
        "   \\\\\n",
        "   &=\n",
        "   -\\frac{1}{n+1}\\mathbf{E}_{Z_{1}^{n}}\\left[r(\\overline{U}^{(n)}) \\vert Z_{1}, \\dots, Z_{n}\\right]\n",
        "     + \\mathbf{E}_{Z_{1}^{n+1}} \\left[ \\frac{1}{\\lambda}\\log \\left(\\sum_{i=1}^{m}U^{(n)}_{i} \\right) \\right].\n",
        "  \\end{align}\n",
        "  Repeating the above chain of inequalities a total of $n+1$ times, we obtain that for any $\\lambda \\leq (n+1)/8$ it holds that\n",
        "  \\begin{align}\n",
        "   -\\min_{i=1,\\dots,m} \\mathbf{E}_{Z_{1}^{n+1}} R_{n+1}(e_{i})\n",
        "   &\\leq\n",
        "   -\\frac{1}{n+1}\\sum_{i=0}^{n}\\mathbf{E}\\left[r(\\overline{U}^{(i)}) \\vert Z_{1},\\dots,Z_{n}\\right] + \\frac{1}{\\lambda} \\log\\left(\\sum_{i=1}^{m} U^{(0)}_{i}\\right)\n",
        "   \\\\\n",
        "   &\\leq\n",
        "      -\\mathbf{E}\\left[r\\left(\\frac{1}{n+1}\\sum_{i=0}^{n}\\overline{U}^{(i)}\\right) \\vert Z_{1},\\dots,Z_{n}\\right] + \\frac{1}{\\lambda} \\log\\left(\\sum_{i=1}^{m} U^{(0)}_{i}\\right),\n",
        "  \\end{align}\n",
        "  where the last line follows by convexity of the quadratic loss.\n",
        "  Plugging in $\\lambda = (n+1)/8$, $U^{(0)}_{i} = 1$ and rearranging the above equation yields the desired result:\n",
        "  \\begin{align}\n",
        "   \\mathcal{E}\\left(\\frac{1}{n+1}\\sum_{i=0}^{n}\\overline{U}^{(i)}\\right)\n",
        "   \\leq 8\\frac{\\log m}{n+1}.\n",
        "  \\end{align}\n",
        "\n",
        "  In Learning Theory literature, the estimator\n",
        "  \\begin{equation}\n",
        "    \\tag{6}\n",
        "    W = \\frac{1}{n+1}\\sum_{i=0}^{n}\\overline{U}^{(i)}\n",
        "  \\end{equation}\n",
        "  is known by the names *exponential weights algorithm*, *progressive mixture rule* and *weighted majority algorithm*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMfHhhx-89bI"
      },
      "source": [
        "### Exercise 10\n",
        "\n",
        "- Exercise 10.1. Implement the exponential weights algorithm stated in equation $(6)$ by completing the code in the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIASPSSf0E9y"
      },
      "source": [
        "class ExponentialWeightsEstimator(Estimator):\n",
        "\n",
        "  def __init__(self, mu):\n",
        "    \"\"\" :mu: The rescaled parameter mu = (n+1)*lambda from the notation used in\n",
        "    Exercise 9. Note that `lambda` is a reserved keywork in python. \"\"\"\n",
        "    self.mu = mu\n",
        "\n",
        "  def fit(self, problem):\n",
        "    \"\"\" See the base class documentation. \"\"\"\n",
        "    ############################################################################\n",
        "    # Exercise 10.1 - fill in the code below\n",
        "    \n",
        "    ############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhEdL2VDoinA"
      },
      "source": [
        "- Exercise 10.2. Run the following code cell. Investigate the experiment output stored in the `ewa_errors` variable.\n",
        "  - Do the experimental results suggest that the exponential weights algorithm achieve the **expectation-optimal** estimation error rate?\n",
        "  - Do the experimental results suggest that the exponential weights algorithm achieve the **deviation-optimal** estimation error rate?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GScRGELpA6H"
      },
      "source": [
        "biased_coin_problems = [BiasedCoinProblem(n=n, m=2, gamma=0.5/np.sqrt(n)) \\\n",
        "                        for n in np.arange(100, 200, step=10)]\n",
        "biased_coin_experiment = ConvergenceRateExperiment(biased_coin_problems)\n",
        "ewa_errors = biased_coin_experiment.sample_estimation_errors(n_seeds=10000, estimator=ExponentialWeightsEstimator(mu=1.0/8.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7CcavuQDb0w"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q64h_ecEDeu5"
      },
      "source": [
        "- Exercise 10.1.\n",
        "```\n",
        "    # First compute an m \\times n losses, where losses_{i,j} is the quadratic\n",
        "    # error of the estimator a_{i} on the j-th data point (x_{j}, y_{j}).\n",
        "    losses = np.zeros((len(problem.A), problem.n))\n",
        "    for idx, a in enumerate(problem.A):\n",
        "      losses[idx, :] = ((a(problem.X) - problem.y).flatten())**2\n",
        "\n",
        "    # Below we follow the notation used in the proof of Exercise 9.2.\n",
        "    u_0 = np.ones(len(problem.A))\n",
        "    overline_u_0 = u_0 / np.sum(u_0)\n",
        "    w = overline_u_0\n",
        "\n",
        "    cummulative_losses = np.cumsum(losses, axis=1)\n",
        "    for i in range(problem.n):\n",
        "\n",
        "      # Subtract the maximum from cummulative losses to prevent underflows and\n",
        "      # overflows. Note that subtracting a constant from cummulative losses\n",
        "      # does not affect the computation of the weights \\overline{U}^{(i)}.\n",
        "      cummulative_losses[:,i] -= np.max(cummulative_losses[:,i])\n",
        "      u_i = np.exp(-self.mu * cummulative_losses[:,i])\n",
        "      overline_u_i = u_i / np.sum(u_i)\n",
        "      w += overline_u_i\n",
        "      \n",
        "    w /= (problem.n + 1)\n",
        "    return w\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwLcGq2WD9uL"
      },
      "source": [
        "- Exercise 10.2. To verify that the exponential weights algorithm achieves the expecatation optimal deviation rate we may compute the means along the first axis in the `ewa_errors` variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkFxxae8pmxf"
      },
      "source": [
        "print(np.mean(ewa_errors,axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCteP6MAprY6"
      },
      "source": [
        "As the outputted means are negative, the experimental results verify that the exponential weights algorithm attains the expectation optimal rate for this problem instance. This is not unexpected as we have established such result theoretically in Exercise 9.\n",
        "\n",
        "Next, we investigate the tail-behavious of the experiments output. Let us sort the `ewa_errors` variable and plot the outputs for three different experimental setups (corresponding to different choices of `n` and `gamma`). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU0hUo3RHKH0"
      },
      "source": [
        "sorted_ewa_errors = np.sort(ewa_errors, axis=1)\n",
        "plt.plot(sorted_ewa_errors[0,:])\n",
        "#plt.plot(sorted_ewa_errors[4,:])\n",
        "#plt.plot(sorted_ewa_errors[-1,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQUt1D0NqcI6"
      },
      "source": [
        "<font color='green'>**The plot generated in the above cell suggest that the exponential weights algorithm is not deviation optimal as on an event of low probability it incurs an estimation error of similar order as the parameter `gamma`**</font>. Indeed, let us average the estimation error over $10$ worst outcomes for each experimental setup and generate a log-log plot versus the sample size $n$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBk5jTNbrSfE"
      },
      "source": [
        "worst_outcomes = np.mean(sorted_ewa_errors[:,-10:], axis=1)\n",
        "plt.plot(np.log(biased_coin_experiment.ns), np.log(worst_outcomes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5Z9IY6Jr3O5"
      },
      "source": [
        "The above plot suggests that on an event of low probability, the exponential weights algorithm incurs the \"slow rate\" estimation error of oder $1/\\sqrt{n}$. This is indeed the case; see the bibliographic remarks section for references."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b44aunwxpTop"
      },
      "source": [
        "## The Star/Midpoint Estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNxLvHS6attB"
      },
      "source": [
        "<font color='green'>**The next exercise is designed to guide you through the development of (one of many) deviation-optimal aggregation procedures. The key technical tool in proving the desired bound is Bernstein's concentration inequality. The key property of the problem allowing for fast rate of order $O(1/n)$ is the curvature of the quadratic loss.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN9v8n-JRu4K"
      },
      "source": [
        "### Exercise 11\n",
        "Recall that we work under the constraint $\\mathcal{Y} = [-1, 1]$. For any function $a : \\mathcal{X} \\to \\mathcal{Y}$ we introduce the following shorthand notation for population and empirical $L_{2}$ norms:\n",
        "\\begin{align*}\n",
        "  \\|a\\|_{2}^{2} &= \\|a\\|_{L_{2}(P)}^{2} = \\mathbf{E}[a(X)^{2}],\\\\\n",
        "  \\|a\\|_{n}^{2} &= \\frac{1}{n} \\sum_{i=1}^{n} a(X_{i})^{2}.\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u1rA-Ybi5Qo"
      },
      "source": [
        "#### Exercise 11.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nenmUMeEi7bH"
      },
      "source": [
        "Prove that for any two functions $a, a' : \\mathcal{X} \\to \\mathcal{Y}$ the following inequality holds with probability at least $1 - \\delta$:\n",
        "$$\n",
        "  r(a) - r(a') \\leq R(a) - R(a') +\n",
        "  c\\left(\\sqrt{\\frac{\\|a-a'\\|_{n}^{2} \\log(2/\\delta)}{n}} + \\frac{\\log(2/\\delta)}{n} \\right),\n",
        "$$\n",
        "where $c$ is some absolute constant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyXMuWWnjtrJ"
      },
      "source": [
        "##### Hint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5jkS0rVj3oe"
      },
      "source": [
        "Consider the following decomposition:\n",
        "\\begin{align*}\n",
        "  &(r(a) - r(a')) - (R(a) - R(a'))\n",
        "  \\\\\n",
        "  &= \\left( \\|a - a'\\|_{2}^{2} + 2\\mathbf{E}[(a - a')(a' - Y)]\\right)\n",
        "  - \\left( \\|a - a'\\|_{n}^{2} + \\frac{2}{n}\\sum_{i=1}^{n}(a(X_{i}) - a'(X_{i}))(a'(X_{i}) - Y_{i})\\right)\n",
        "  \\\\\n",
        "  &\\leq\n",
        "  \\underbrace{\\|a - a'\\|_{2}^{2} - \\|a - a'\\|_{n}^{2}}_{T_{1}}\n",
        "  +\n",
        "  2\\cdot\\underbrace{\\mathbf{E}[(a - a')(a' - Y)] - \\frac{1}{n}\\sum_{i=1}^{n}(a(X_{i}) - a'(X_{i}))(a'(X_{i}) - Y_{i})\n",
        "  }_{T_{2}}.\n",
        "\\end{align*}\n",
        "\n",
        "Apply Bernstein's inequality to the terms $T_1$ and $T_2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ty6U2tMlH5r"
      },
      "source": [
        "##### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY18VxrulKK6"
      },
      "source": [
        "Since $\\mathcal{Y} = [-1, 1]$ we have\n",
        "$\\mathbf{E}[(a(X) - a'(X))^{4}] \\leq 4 \\mathbf{E}[(a(X) - a'(X))^{2}] = 4\\|a-a'\\|_{2}^{2}$\n",
        "and $\\mathbf{E}[(a(X) - a'(X))^{2}(a'(X) - Y)^{2}] \\leq 4 \\mathbf{E}[(a(X) - a'(X))^{2}] = 4\\|a - a'\\|^{2}$. Define the events\n",
        "$$\n",
        "  E_{1} = \\left\\{    T_{1} \\geq \\sqrt{\\frac{8\\|a - a'\\|_{2}^{2}\\log(2/  \\delta)}{n}}\n",
        "    + \\frac{4\\log(2/\\delta)}{3n}\n",
        "    \\right\\},\n",
        "    \\quad\n",
        "  E_{2} = \\left\\{    T_{2} \\sqrt{\\frac{8\\|a - a'\\|_{2}^{2}\\log(2/  \\delta)}{n}}\n",
        "    + \\frac{2\\log(2/\\delta)}{3n}\n",
        "    \\right\\}.\n",
        "$$\n",
        "Notice that on the event $E_{1}^{c}$ we have\n",
        "\\begin{align*}\n",
        "  &\\|a-a'\\|_{2}^{2} \\leq \\|a-a'\\|_{n}^{2} + \n",
        "  \\sqrt{\\frac{8\\|a - a'\\|_{2}^{2}\\log(2/  \\delta)}{n}}\n",
        "    + \\frac{4\\log(2/\\delta)}{3n}\n",
        "    \\\\\n",
        "    \\implies&\n",
        "    \\|a-a'\\|_{2}^{2} \\leq 2\\|a-a'\\|_{n}^{2} + \n",
        "    \\underbrace{\n",
        "  \\sqrt{\\frac{8\\|a - a'\\|_{2}^{2}\\log(2/  \\delta)}{n}}\n",
        "  - \\|a-a'\\|_{2}^{2}\n",
        "  }_{\\leq (2\\log(2/\\delta))/n \\text{ by optimizing with respect to }\\|a-a'\\|_{2}}\n",
        "    + \\frac{4\\log(2/\\delta)}{3n}\n",
        "    \\\\\n",
        "    \\implies&\n",
        "    \\|a-a'\\|_{2}^{2} \\leq 2\\|a-a'\\|_{n}^{2}\n",
        "    + \\frac{10\\log(2/\\delta)}{3n}.\n",
        "\\end{align*}\n",
        "Thus, on the event $(E_{1} \\cup E_{2})^{c}$ we have\n",
        "\\begin{align*}\n",
        "  r(a) - r(a') - (R(a) - R(a'))\n",
        "  &= T_{1} + 2T_{2}\n",
        "  \\\\\n",
        "  &\\leq\n",
        "  3\\sqrt{\\frac{8\\|a - a'\\|_{2}^{2}\\log(2/  \\delta)}{n}}\n",
        "    + \\frac{8\\log(2/\\delta)}{3n}\n",
        "  \\\\\n",
        "  &\\leq\n",
        "  3\\sqrt{\\frac{16\\|a - a'\\|_{n}^{2}\\log(2/  \\delta)}{n} + \\frac{80 \\log(2/\\delta)^{2}}{3n^2}}\n",
        "    + \\frac{8\\log(2/\\delta)}{3n}\n",
        "  \\\\\n",
        "  &\\leq\n",
        "  12\\sqrt{\\frac{\\|a - a'\\|_{n}^{2}\\log(2/  \\delta)}{n}}\n",
        "  + \\left(\\sqrt{\\frac{80}{3}} + \\frac{8}{3}\\right)\\frac{\\log(2/\\delta)}{n}\n",
        "  \\\\\n",
        "  &\\leq\n",
        "  12\\sqrt{\\frac{\\|a - a'\\|_{n}^{2}\\log(2/  \\delta)}{n}}\n",
        "  + 8\\frac{\\log(2/\\delta)}{n}.\n",
        "\\end{align*}\n",
        "By Bernstein's inequality we have\n",
        "$$\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left(\n",
        "    E_{1}\n",
        "  \\right)\n",
        "  \\leq \\delta/2\n",
        "  \\quad\\text{and}\\quad\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left(\n",
        "    E_{2}\n",
        "  \\right)\n",
        "  \\leq \\delta/2.\n",
        "$$\n",
        "In particular,\n",
        "$$\n",
        "  \\mathbb{P}((E_{1} \\cup E_{2})^{c})\n",
        "  = 1 - \\mathbb{P}(E_{1} \\cup E_{2})\n",
        "  \\geq 1 - \\mathbb{P}(E_{1}) - \\mathbb{P}(E_{2})\n",
        "  \\geq 1 - \\delta.\n",
        "$$\n",
        "The proof is thus complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu36ojslRyAc"
      },
      "source": [
        "#### Exercise 11.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xH5HPg0R3k7"
      },
      "source": [
        "We will now consider a simplified setting of model selection aggregation problem with $m = 2$, that is, $\\mathcal{A} = \\{a_{1}, a_{2}\\}$. In Exercise 11.3 we will design an improper learning algorithm that outputs a function in the enlarged class $\\overline{A} = \\{a_{1}, a_{2}, (a_{1} + a_{2})/2\\}$ and achieves the deviation-optimal estimation error rate.\n",
        "\n",
        "The intuition to additionally consider the midpoint $(a_{1} + a_{2})/2$ comes from the following equation (recall the computations provided in the hint for Exercise 2.2):\n",
        "$$\n",
        "  r((a_{1} + a_{2})/2) = \\frac{1}{2}(r(a_{1}) + r(a_{2})) - \\frac{1}{4}\\|a_{1} - a_{2}\\|_{2}^{2}.\n",
        "$$\n",
        "In particular, if $\\|a_{1} - a_{2}\\|_{2}^{2}$ is sufficiently large, the midpoint\n",
        "$(a_{1} + a_{2})/2$ might incur a significantly smaller risk than either $a_{1}$ or $a_{2}$.\n",
        "\n",
        "By completing the code cell below, you are first asked to provide empirical evidence that empirical risk minimization over $\\overline{\\mathcal{A}}$ fails to achieve the expectation-optimal (and hence also deviation-optimal) estimation error rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqYzDGxOA2Ek"
      },
      "source": [
        "class ImproperERM(Estimator):\n",
        "  \"\"\" An implementation of the (improper) empirical risk minimization algorithm.\n",
        "  \"\"\"\n",
        "\n",
        "  def fit(self, problem):\n",
        "    # In this problem we only consider m = 2.\n",
        "    assert len(problem.A) == 2\n",
        "\n",
        "    erm = ERM()\n",
        "    erm_w = erm.fit(problem)\n",
        "    erm_idx = np.argmax(erm_w)\n",
        "    erm_R = problem.compute_empirical_risk(problem.A[erm_idx])\n",
        "    # We will not check if the midpoint (a_1 + a_2)/2 achieves better empirical\n",
        "    # risk.\n",
        "    def midpoint_a(X):\n",
        "      return (problem.A[0](X) + problem.A[1](X))/2.0\n",
        "    midpoint_R = problem.compute_empirical_risk(midpoint_a)\n",
        "\n",
        "    if midpoint_R < erm_R:\n",
        "      # The midpoint (a_1 + a_2)/2 achieves the smallest empirical risk.\n",
        "      return np.ones(2)/2.0\n",
        "    else:\n",
        "      # One of the functions in {a_1, a_2} achieves the smallest empirical risk.\n",
        "      return erm_w\n",
        "\n",
        "################################################################################\n",
        "# Exercise 11.2. Implement a class `HardPorblemInstanceForImproperERM` that\n",
        "# contains only two functions for which the ImproperERM estimator incurs\n",
        "# suboptimal in-expectation estimation error rate.\n",
        "\n",
        "################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msY5NGZsGQU5"
      },
      "source": [
        "Verify your implementation by running the code cell below. The slope in the plot generated above should be approximately -0.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2wzXTbLGeD_"
      },
      "source": [
        "improper_erm_problems = [HardProblemForImproperERM(n=n) \\\n",
        "                        for n in np.arange(100, 200, step=10)]\n",
        "improper_erm_experiment = ConvergenceRateExperiment(improper_erm_problems)\n",
        "improper_erm_errors = improper_erm_experiment.sample_estimation_errors(\n",
        "    n_seeds=1000, estimator=ImproperERM())\n",
        "# To empirically establish that the improper ERM algorithm is **not**\n",
        "# expecatation optimal, the slope in the graph generated above should be larger\n",
        "# than -1.0 (e.g., slope -0.5 would correspond to the ``slow rate'').\n",
        "plt.plot(np.log(improper_erm_experiment.ns),\n",
        "         np.log(np.mean(improper_erm_errors, axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpe58efMGRPK"
      },
      "source": [
        "##### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH9HZ1XnC_hy"
      },
      "source": [
        "The idea is to construct a problem that is just as hard for the `ImproperERM` as the `BiasedCoinProblem` is for the `ERMEstimator`.\n",
        "\n",
        "Suppose that the ERM estimator does not attain the fast rate for a problem with\n",
        "dictionary $\\mathcal{A} = \\{a_{1}, a_{2}\\}$. It thus suffices to construct a problem class $\\tilde{\\mathcal{A}} = \\{a_{1}, \\tilde{a}_{3}\\}$ with $r(\\tilde{a}_{3}) \\gg \\max\\{r(a_{1}), r(a_{2})\\}$ and $\\frac{a_{1} + \\tilde{a}_{3}}{2} = a_{2}$. For such a problem instance, the `ImproperERM` estimator will ignore the function $\\tilde{a}_{3}$ with high probability and hence the `ImproperERM` estimator applied to $\\tilde{\\mathcal{A}}$ will behave as the (proper) ERM estimator applied to the class $\\mathcal{A}$.\n",
        "\n",
        "We provide a sample implementation of this idea below:\n",
        "```\n",
        "class HardProblemForImproperERM(BiasedCoinProblem):\n",
        "  \"\"\" A hard problem instance on which ImproperERM estimator fails to achieve\n",
        "  the expectation optimal estimation error rate. \"\"\"\n",
        "  def __init__(self, n):\n",
        "    super().__init__(n=n, m=2, gamma=0.5/np.sqrt(n))\n",
        "\n",
        "    A = self.A.copy()\n",
        "    # Replace the function A[1] (i.e., the function that represents an unbiased\n",
        "    # coin) by a new function `endpoint_a` such that the function\n",
        "    # A[1] = (A[0] + endpoint_a)/2 will be considered by the ImproperERM\n",
        "    # estimator implemented above.\n",
        "    def endpoint_a(X):\n",
        "      return 2*A[1](X) - A[0](X)\n",
        "    self.A[1] = endpoint_a\n",
        "    self.r[1] = 6.0 + 4.0*self.gamma\n",
        "\n",
        "  # Since we have modified the dictionary of the base class, we need to\n",
        "  # adjust the implementation of computation of L2 squared norms.\n",
        "  def compute_L2_squared_norm(self, w):\n",
        "    return (w[0] - w[1])**2 + (2.0 * w[1])**2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJqs04pqR1Nn"
      },
      "source": [
        "#### Exercise 11.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6N8iXYuHz2g"
      },
      "source": [
        "Using the intuition gained in Exercises 11.1 and 11.2, propose a deviation-optimal model selection aggregation algorithm that outputs a function in the set\n",
        "$$\n",
        "  (\\mathcal{A} + \\mathcal{A})/2 = \\{(a + a')/2 : a,a' \\in \\mathcal{A}\\}.\n",
        "$$\n",
        "\n",
        "Your algorithm should work for any dictionary size $m > 2$ and it is allowed to be parametrized by the desired confidence level $\\delta$. That is, given $\\delta \\in (0, 1)$, propose a procedure $W_{\\delta}$ such that\n",
        "$$\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\n",
        "  \\left(\n",
        "    \\mathcal{E}(W_{\\delta}) \\geq c\\frac{\\log(m/\\delta)}{n}\n",
        "  \\right)\n",
        "  \\leq \\delta.\n",
        "$$\n",
        "for some absolute constant $c$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MMvjehrKj75"
      },
      "source": [
        "##### Hint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZsCFZS-KmtA"
      },
      "source": [
        "First consider the case $m=2$. When does empirical risk minimization over $\\mathcal{A}$ yield a deviation optimal algorithm? When does it make sense to consider the midpoint $(a_{1} + a_{2})/2$ in addition to the class $\\mathcal{A} = \\{a_{1}, a_{2}\\}$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZov5IZyLPN2"
      },
      "source": [
        "##### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKfLK-h4RmvP"
      },
      "source": [
        "Let $a^{\\star}$ denote the population risk minimizer among $\\mathcal{A}$.\n",
        "For any function $a : \\mathcal{X} \\to \\mathcal{Y}$, define the event\n",
        "$$\n",
        "  E_{a} = \\left\\{r(a) - r(a^{\\star}) \\leq R(a) - R(a^{\\star})\n",
        "  +  12\\sqrt{\\frac{\\|a - a^{\\star}\\|_{n}^{2}\\log(2m^{2}/\\delta)}{n}}\n",
        "  + 8\\frac{\\log(2m^{2}/\\delta)}{n}\\right\\}\n",
        "$$\n",
        "and let\n",
        "$$\n",
        "  E = \\cup_{a \\in (\\mathcal{A} + \\mathcal{A})/2} E_{a}.\n",
        "$$\n",
        "By Exercise 11.1, we have\n",
        "$$\n",
        "  \\mathbb{P}_{Z_{1}^{n}}\\left( E \\right) \\geq 1 - \\delta.\n",
        "$$\n",
        "In the rest of what follows we work conditionally on the event $E$.\n",
        "\n",
        "\n",
        "\n",
        "Let $A^{(ERM)} \\in \\mathrm{argmin}_{a \\in \\mathcal{A}} R(a)$ denote any empirical risk minimizer over $\\mathcal{A}$. Given the desired confidence parameter $\\delta$, define the random set of \"almost empirical risk minimizers'' by\n",
        "$$\n",
        "  \\widehat{\\mathcal{A}}_{\\delta} = \\left\\{\n",
        "    a \\in \\mathcal{A} : \n",
        "    R(a) \\leq R(A^{(ERM)}) + 12\\sqrt{\\frac{\\|A^{(ERM)} - a\\|_{n}^{2}\\log(2m^{2}/\\delta)}{n}}\n",
        "     + 8\\frac{\\log(2m^{2}/\\delta)}{n}\n",
        "  \\right\\}.\n",
        "$$\n",
        "<font color='green'>**We now define our estimator $A^{(MID)}$ as an empirical risk minimizer over the set\n",
        "$(A^{(ERM)} + \\widehat{\\mathcal{A}}_{\\delta})/2$, that is:**</font>\n",
        "$$\n",
        "  A^{(MID)} \\in \\mathrm{argmin}_{a \\in \\widehat{\\mathcal{A}}_{\\delta}} R\\left(\n",
        "    \\frac{A^{(ERM)} + a}{2}\\right).\n",
        "$$\n",
        "Let $A'$ denote the element in $\\widehat{\\mathcal{A}}_{\\delta}$\n",
        "$$\n",
        "  A' = \\mathrm{argmax}_{a \\in \\widehat{\\mathcal{A}}_{\\delta}}\\|A^{(ERM)} - a\\|_{n}^{2}\n",
        "  \\quad\\text{and let}\\quad\n",
        "  D = \\|A^{(ERM)} - A'\\|_{n}^{2}.\n",
        "$$\n",
        "Notice that on the event $E$, using the fact that\n",
        "$r(a^{\\star}) \\leq r(A^{(ERM)})$ it holds that\n",
        "$$\n",
        "  a^{\\star} \\in \\widehat{\\mathcal{A}}_{\\delta}\n",
        "  \\quad\\text{and in particular,}\\quad\n",
        "  \\|A^{(MID)} - a^{\\star}\\|_{n}^{2} \\leq \\frac{9}{4}D.\n",
        "$$\n",
        "Hence, on the event $E$ we have\n",
        "\\begin{align*}\n",
        "  \\mathcal{E}(A^{(MID)})\n",
        "  &= r(A^{(MID)}) - r(a^{\\star})\n",
        "  \\\\\n",
        "  &\\leq R(A^{(MID)}) - R(a^{\\star})\n",
        "  + 12\\sqrt{\\frac{\\|A^{(MID)} - a^{\\star}\\|_{n}^{2}\\log(2m^{2}/\\delta)}{n}}\n",
        "     + 8\\frac{\\log(2m^{2}/\\delta)}{n}\n",
        "  \\\\\n",
        "  &\\leq R(A^{(MID)}) - R(a^{\\star})\n",
        "  + 18\\sqrt{\\frac{D\\log(2m^{2}/\\delta)}{n}}\n",
        "     + 8\\frac{\\log(2m^{2}/\\delta)}{n}\n",
        "  \\\\\n",
        "  &\\leq R\\left(\\frac{A^{(ERM)} + A'}{2}\\right) - R(a^{\\star})\n",
        "  + 18\\sqrt{\\frac{D\\log(2m^{2}/\\delta)}{n}}\n",
        "     + 8\\frac{\\log(2m^{2}/\\delta)}{n}\n",
        "  \\\\\n",
        "  &= \\frac{1}{2}R(A^{(ERM)}) + \\frac{1}{2}R(A') - \\frac{1}{4}D - R(a^{\\star})\n",
        "  + 18\\sqrt{\\frac{D\\log(2m^{2}/\\delta)}{n}}\n",
        "     + 8\\frac{\\log(2m^{2}/\\delta)}{n}\n",
        "  \\\\\n",
        "  &\\leq \\underbrace{R(A^{(ERM)}) - R(a^{\\star})}_{\\leq 0}\n",
        "  + \\underbrace{24\\sqrt{\\frac{D\\log(2m^{2}/\\delta)}{n}}\n",
        "  - \\frac{1}{4}D}_{\\leq 576\\log(2m^{2}/\\delta)/n \\text{ by optimizing with respect to }\\sqrt{D}}\n",
        "     + 12\\frac{\\log(2m^{2}/\\delta)}{n}\n",
        "  \\\\\n",
        "  &\\leq\n",
        "  588\\frac{\\log(2m^{2}/\\delta)}{n}.\n",
        "\\end{align*}\n",
        "Our proof is complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEWVr1-wSMN0"
      },
      "source": [
        "### Exercise 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex2Oc1mUMR9v"
      },
      "source": [
        "Implement the algorithm described in the solution of Exercise 11.3 by filling in the missing code in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s40NczR9pVv6"
      },
      "source": [
        "class MidpointEstimator(Estimator):\n",
        "\n",
        "  def __init__(self, delta):\n",
        "    \"\"\" :delta: The desired confidence level. \"\"\"\n",
        "    self.delta = delta\n",
        "\n",
        "  def fit(self, problem):\n",
        "    ############################################################################\n",
        "    # Exercise 12. Fill in the implementation details below.\n",
        "    \n",
        "    ############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQag5iyIN5b5"
      },
      "source": [
        "In the next code cell we repeat the experiment used to empirically suggest deviation-suboptimality of the exponential weights algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ7McAlDsc10"
      },
      "source": [
        "biased_coin_problems = [BiasedCoinProblem(n=n, m=2, gamma=0.5/np.sqrt(n)) \\\n",
        "                        for n in np.arange(100, 200, step=10)]\n",
        "biased_coin_experiment = ConvergenceRateExperiment(biased_coin_problems)\n",
        "midpoint_estimator_errors = biased_coin_experiment.sample_estimation_errors(\n",
        "    n_seeds=10000, estimator=MidpointEstimator(delta=1.0/10000))\n",
        "\n",
        "# The maximumum estimation error printed below should be 0.\n",
        "print(\"Maximum estimation error:\", np.max(midpoint_estimator_errors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XABtc1uDhxj"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK7f72AxDjpX"
      },
      "source": [
        "The algorithm described in the solution of Exercise 11.3 can be implemented as\n",
        "follows:\n",
        "```\n",
        "    # First find an empirical risk minimizer over the class A.\n",
        "    erm = ERM()\n",
        "    erm_w = erm.fit(problem)\n",
        "    erm_idx = np.argmax(erm_w)\n",
        "\n",
        "    # We now implement a function to check if a function `a` belongs to the\n",
        "    # set of almost minimizers \\in \\widehat{A}_{\\delta}.\n",
        "    R_erm = problem.compute_empirical_risk(problem.A[erm_idx])\n",
        "    m = len(problem.A)\n",
        "    a_erm_X = problem.A[erm_idx](problem.X) \n",
        "    def is_almost_minimizer(a):\n",
        "      a_X = a(problem.X)\n",
        "      l2_dist = np.average((a_erm_X - a_X)**2)\n",
        "      log_term = np.log(2.0*m**2/self.delta)/problem.n\n",
        "      R_a = problem.compute_empirical_risk(a)\n",
        "\n",
        "      if R_a <= R_erm + 12.0 * np.sqrt(l2_dist * log_term) + 8.0 * log_term:\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "\n",
        "    # We will now loop through the elements of problem.A and consider the\n",
        "    # midpoints between A^(ERM) and almost minimizers as potential candidate\n",
        "    # functions.\n",
        "    for endpoint_idx in range(len(problem.A)):\n",
        "      if not is_almost_minimizer(problem.A[endpoint_idx]):\n",
        "        # The function a_{endpoint_idx} is not in the set of almost minimizers\n",
        "        # of the empirical risk.\n",
        "        continue\n",
        "      \n",
        "      # Define the midpoint function.\n",
        "      def midpoint_a(X):\n",
        "        return (problem.A[erm_idx](X) + problem.A[endpoint_idx](X))/2.0\n",
        "      R[endpoint_idx] = problem.compute_empirical_risk(midpoint_a)\n",
        "      \n",
        "    best_endpoint_idx = np.argmin(R)\n",
        "    mid_w = erm_w / 2.0\n",
        "    mid_w[best_endpoint_idx] += 0.5\n",
        "    return mid_w\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODMqQrNupaUa"
      },
      "source": [
        "## Bibliographic Remarks\n",
        "\n",
        "The problem of model selection aggregation, among closely related problems of convex and linear aggregation, were studied in the context of non-parametric statistics by *Nemirovski [2000]*. Minimax optimal rates of aggregation for all three problems were established by *Tsybakov [2003]*; see the book by *Tsybakov [2008]* for an introduction to the techniques used to prove information-theoretic lower bounds. In statistics literature, the exponential weights algorithm was used for the model selection aggregation problem, among others, by *Catoni [1997]* and *Yang [2004]*. The exponential weights algorithm itself, however, has much older roots in sequential prediction and game theory; see the textbook by *Cesa-Bianchi and Lugosi [2006]* for historical remarks and further references. It was first established that the exponential weights algorithm is deviation suboptimal by *Audibert [2008]*, where the first deviation-optimal procedure was proposed. Further deviation-optimal procedures\n",
        "were introduced by *Lecu and Mendelson [2009], Lecu and Rigollet [2014]* and *Wintenberger [2017]*.\n",
        "\n",
        "**References**\n",
        "\n",
        "J.-Y. Audibert. Progressive mixture rules are deviation suboptimal. In Advances in Neural Information Processing Systems, pages 4148, 2008.\n",
        "\n",
        "O. Catoni. The mixture approach to universal model selection. In cole Normale Suprieure. Citeseer, 1997.\n",
        "\n",
        "N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge university press, 2006.\n",
        "\n",
        "G. Lecu and S. Mendelson. Aggregation via empirical risk minimization. Probability theory and related fields, 145(3-4):591613, 2009.\n",
        "\n",
        "G. Lecu and P. Rigollet. Optimal learning with q-aggregation. Annals of Statistics, 42(1): 211224, 2014.\n",
        "\n",
        "A. Nemirovski. Topics in non-parametric statistics. Ecole dEt de Probabilits de Saint-Flour, 28:85, 2000.\n",
        "\n",
        "A. B. Tsybakov. Optimal rates of aggregation. Conference on Learning Theory, pages 303313, 2003.\n",
        "\n",
        "A. B. Tsybakov. Introduction to nonparametric estimation. Springer Science & Business Media, 2008.\n",
        "\n",
        "O. Wintenberger. Optimal learning with bernstein online aggregation. Machine Learning, 106(1):119141, 2017.\n",
        "\n",
        "Y. Yang. Aggregating regression procedures to improve performance. Bernoulli, 10(1): 2547, 2004."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E35phX6ooXN"
      },
      "source": [
        "## Acknowledgements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od_qd4pHorMw"
      },
      "source": [
        "We thank Nikita Zhivotovskiy for pointing out the short proof of deviation-optimal aggregation via midpoint estimator described in the solution of Exercise 11.3."
      ]
    }
  ]
}